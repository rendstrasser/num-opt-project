{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python394jvsc74a57bd01966315138f400f62dc1fff91c72ed91dd6df0f36082b426fd0f91ceb8258b5f",
   "display_name": "Python 3.9.4 64-bit"
  },
  "metadata": {
   "interpreter": {
    "hash": "1966315138f400f62dc1fff91c72ed91dd6df0f36082b426fd0f91ceb8258b5f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Numerical optimization project\n",
    "by Raphael-Pascal Endstrasser (K11907909)\n",
    "\n",
    "10/10 problems were solved"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# As we are using additional .py files, enable their reloading without restarting the kernel\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from IPython.display import Code\n",
    "\n",
    "from src import algorithms as alg\n",
    "from src.algorithms import MinimizationProblem\n",
    "from src.problems import ROSENBROCK_PROBLEM, create_quadratic_problem, create_non_quadratic_problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "direction_methods = {\n",
    "    \"Steepest Descent\": alg.steepest_descent_direction,\n",
    "    \"Newton\": alg.newton_direction,\n",
    "    \"BFGS Quasi-Newton\": alg.bfgs_quasi_newton_direction,\n",
    "    \"Fletcher-Reeves conjugate\": alg.fr_conjugate_direction,\n",
    "}"
   ]
  },
  {
   "source": [
    "# Rosenbrock function (just for fun)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Rosenbrock problem - minimizer=[1.00000882 1.0000177 ] with f(x)=7.81961948198283e-11 (direction method = Steepest Descent)\nRosenbrock problem - minimizer=[1. 1.] with f(x)=9.575292275185794e-28 (direction method = Newton)\nRosenbrock problem - minimizer=[1. 1.] with f(x)=4.968369118571452e-23 (direction method = BFGS Quasi-Newton)\nRosenbrock problem - minimizer=[1.00000203 1.00000408] with f(x)=4.156431695215182e-12 (direction method = Fletcher-Reeves conjugate)\n"
     ]
    }
   ],
   "source": [
    "x0 = [5.2, 5.2]\n",
    "\n",
    "for name, direction_method in direction_methods.items():\n",
    "    x_minimizer, _ = alg.find_minimizer(ROSENBROCK_PROBLEM, x0, direction_method, max_iter=100_000)\n",
    "    print(f\"Rosenbrock problem - minimizer={x_minimizer} with f(x)={ROSENBROCK_PROBLEM.f(x_minimizer)} (direction method = {name})\")"
   ]
  },
  {
   "source": [
    "# 5 quadratic problems (of the form Ax-b=0)\n",
    "First, we randomly choose a size n between 10 and 20.\n",
    "Then, we create random matrices $A_{gen}$ with integers between 1 and 10 and create our matrix $A=A_{gen} A_{gen}^T$, which is then guaranteed to be positive definite. We create a random vector x with numbers between 1 and 10."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Starting minimization procedure for quadratic problems ...\n",
      "\n",
      "### Minimizing quadratic problem with solution: x = [4 3 4 2 1 3 8 2 2 2], n = 10.\n",
      "\n",
      "Applying minimization procedure with direction method: Steepest Descent\n",
      "Found the minimizer=[4. 3. 4. 2. 1. 3. 8. 2. 2. 2.] with f(x)=-113172.5 after 85268 steps.\n",
      "\n",
      "Applying minimization procedure with direction method: Newton\n",
      "Found the minimizer=[4. 3. 4. 2. 1. 3. 8. 2. 2. 2.] with f(x)=-113172.5 after 2 steps.\n",
      "\n",
      "Applying minimization procedure with direction method: BFGS Quasi-Newton\n",
      "Found the minimizer=[4. 3. 4. 2. 1. 3. 8. 2. 2. 2.] with f(x)=-113172.5 after 21 steps.\n",
      "\n",
      "Applying minimization procedure with direction method: Fletcher-Reeves conjugate\n",
      "Found the minimizer=[4. 3. 4. 2. 1. 3. 8. 2. 2. 2.] with f(x)=-113172.5 after 300000 steps.\n",
      "\n",
      "### Minimizing quadratic problem with solution: x = [2 4 3 8 4 9 7 8 5 9 4 2 4], n = 13.\n",
      "\n",
      "Applying minimization procedure with direction method: Steepest Descent\n",
      "Found the minimizer=[2. 4. 3. 8. 4. 9. 7. 8. 5. 9. 4. 2. 4.] with f(x)=-835987.5 after 46218 steps.\n",
      "\n",
      "Applying minimization procedure with direction method: Newton\n",
      "Found the minimizer=[2. 4. 3. 8. 4. 9. 7. 8. 5. 9. 4. 2. 4.] with f(x)=-835987.5 after 2 steps.\n",
      "\n",
      "Applying minimization procedure with direction method: BFGS Quasi-Newton\n",
      "Found the minimizer=[2. 4. 3. 8. 4. 9. 7. 8. 5. 9. 4. 2. 4.] with f(x)=-835987.5 after 22 steps.\n",
      "\n",
      "Applying minimization procedure with direction method: Fletcher-Reeves conjugate\n",
      "Found the minimizer=[2. 4. 3. 8. 4. 9. 7. 8. 5. 9. 4. 2. 4.] with f(x)=-835987.5 after 300000 steps.\n",
      "\n",
      "### Minimizing quadratic problem with solution: x = [4 4 7 2 3 1 3 9 6 1 6 5], n = 12.\n",
      "\n",
      "Applying minimization procedure with direction method: Steepest Descent\n",
      "Found the minimizer=[4. 4. 7. 2. 3. 1. 3. 9. 6. 1. 6. 5.] with f(x)=-376312.5 after 19339 steps.\n",
      "\n",
      "Applying minimization procedure with direction method: Newton\n",
      "Found the minimizer=[4. 4. 7. 2. 3. 1. 3. 9. 6. 1. 6. 5.] with f(x)=-376312.5 after 2 steps.\n",
      "\n",
      "Applying minimization procedure with direction method: BFGS Quasi-Newton\n",
      "Found the minimizer=[4. 4. 7. 2. 3. 1. 3. 9. 6. 1. 6. 5.] with f(x)=-376312.5 after 19 steps.\n",
      "\n",
      "Applying minimization procedure with direction method: Fletcher-Reeves conjugate\n",
      "Found the minimizer=[4. 4. 7. 2. 3. 1. 3. 9. 6. 1. 6. 5.] with f(x)=-376312.5 after 300000 steps.\n",
      "\n",
      "### Minimizing quadratic problem with solution: x = [5 1 1 2 2 2 8 5 5 2], n = 10.\n",
      "\n",
      "Applying minimization procedure with direction method: Steepest Descent\n",
      "Found the minimizer=[5. 1. 1. 2. 2. 2. 8. 5. 5. 2.] with f(x)=-110165.0 after 17703 steps.\n",
      "\n",
      "Applying minimization procedure with direction method: Newton\n",
      "Found the minimizer=[5. 1. 1. 2. 2. 2. 8. 5. 5. 2.] with f(x)=-110165.0 after 2 steps.\n",
      "\n",
      "Applying minimization procedure with direction method: BFGS Quasi-Newton\n",
      "Found the minimizer=[5. 1. 1. 2. 2. 2. 8. 5. 5. 2.] with f(x)=-110165.0 after 18 steps.\n",
      "\n",
      "Applying minimization procedure with direction method: Fletcher-Reeves conjugate\n",
      "Found the minimizer=[5. 1. 1. 2. 2. 2. 8. 5. 5. 2.] with f(x)=-110165.0 after 300000 steps.\n",
      "\n",
      "### Minimizing quadratic problem with solution: x = [7 9 6 4 6 7 1 5 7 3 2], n = 11.\n",
      "\n",
      "Applying minimization procedure with direction method: Steepest Descent\n",
      "Found the minimizer=[7. 9. 6. 4. 6. 7. 1. 5. 7. 3. 2.] with f(x)=-486149.0 after 63983 steps.\n",
      "\n",
      "Applying minimization procedure with direction method: Newton\n",
      "Found the minimizer=[7. 9. 6. 4. 6. 7. 1. 5. 7. 3. 2.] with f(x)=-486149.0 after 2 steps.\n",
      "\n",
      "Applying minimization procedure with direction method: BFGS Quasi-Newton\n",
      "Found the minimizer=[7. 9. 6. 4. 6. 7. 1. 5. 7. 3. 2.] with f(x)=-486149.0 after 18 steps.\n",
      "\n",
      "Applying minimization procedure with direction method: Fletcher-Reeves conjugate\n",
      "Found the minimizer=[7. 9. 6. 4. 6. 7. 1. 5. 7. 3. 2.] with f(x)=-486149.0 after 300000 steps.\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(41) # reproducability\n",
    "\n",
    "# take 5 random values of n for matrix/vector size of quadratic problem\n",
    "n_values = np.random.randint(low=10, high=21, size=5)\n",
    "print(\"Starting minimization procedure for quadratic problems ...\")\n",
    "\n",
    "for n in n_values:\n",
    "    problem = create_quadratic_problem(n) # create quadratic minimization problem for n\n",
    "    print(f\"\\n### Minimizing quadratic problem with solution: x = {problem.solution}, n = {n}.\")\n",
    "\n",
    "    x0 = np.zeros(n) # we start with x = 0\n",
    "\n",
    "    for name, direction_method in direction_methods.items():\n",
    "        print(f\"\\nApplying minimization procedure with direction method: {name}\")\n",
    "        x_minimizer, grad_norms = alg.find_minimizer(problem, x0, direction_method, tolerance=1e-5, max_iter=300_000)\n",
    "        x_minimizer = np.around(x_minimizer, 3) # round to 3 decimals to check our definition of \"solved\"\n",
    "        print(f\"Found the minimizer={x_minimizer} with f(x)={problem.f(x_minimizer)} after {len(grad_norms)} steps.\")"
   ]
  },
  {
   "source": [
    "# 5 non-quadratic problems\n",
    "\n",
    "The problem functions are created as the antiderivatives of (x-a)(x-b)(x-c) with a,b,c being random integers within -10 and 10.\n",
    "We choose x0=-20 such that we have convergence for the Newton direction (positive-definite Hessian).\n",
    "Note that we have 2 minimizers and depending on the method, we might find one or the other."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Starting minimization procedure for non-quadratic problems (polynomials of 4th degree) ...\n",
      "\n",
      "### Minimizing non-quadratic problem with solutions: x = -10 or x = 11.\n",
      "\n",
      "Applying minimization procedure with direction method: Steepest Descent\n",
      "Found the minimizer=[-10.] with f(x)=-4100.0 after 37 steps.\n",
      "\n",
      "Applying minimization procedure with direction method: Newton\n",
      "Found the minimizer=[-10.] with f(x)=-4100.0 after 7 steps.\n",
      "\n",
      "Applying minimization procedure with direction method: BFGS Quasi-Newton\n",
      "Found the minimizer=[-10.] with f(x)=-4100.0 after 8 steps.\n",
      "\n",
      "Applying minimization procedure with direction method: Fletcher-Reeves conjugate\n",
      "Found the minimizer=[-10.] with f(x)=-4100.0 after 119 steps.\n",
      "\n",
      "### Minimizing non-quadratic problem with solutions: x = -8 or x = 8.\n",
      "\n",
      "Applying minimization procedure with direction method: Steepest Descent\n",
      "Found the minimizer=[-8.] with f(x)=-682.6666666666666 after 10 steps.\n",
      "\n",
      "Applying minimization procedure with direction method: Newton\n",
      "Found the minimizer=[-8.] with f(x)=-682.6666666666666 after 8 steps.\n",
      "\n",
      "Applying minimization procedure with direction method: BFGS Quasi-Newton\n",
      "Found the minimizer=[-8.] with f(x)=-682.6666666666666 after 9 steps.\n",
      "\n",
      "Applying minimization procedure with direction method: Fletcher-Reeves conjugate\n",
      "Found the minimizer=[-8.] with f(x)=-682.6666666666666 after 6 steps.\n",
      "\n",
      "### Minimizing non-quadratic problem with solutions: x = -7 or x = 14.\n",
      "\n",
      "Applying minimization procedure with direction method: Steepest Descent\n",
      "Found the minimizer=[14.] with f(x)=-6402.666666666666 after 27 steps.\n",
      "\n",
      "Applying minimization procedure with direction method: Newton\n",
      "Found the minimizer=[-7.] with f(x)=-1000.4166666666665 after 8 steps.\n",
      "\n",
      "Applying minimization procedure with direction method: BFGS Quasi-Newton\n",
      "Found the minimizer=[-7.] with f(x)=-1000.4166666666665 after 9 steps.\n",
      "\n",
      "Applying minimization procedure with direction method: Fletcher-Reeves conjugate\n",
      "Found the minimizer=[14.] with f(x)=-6402.666666666666 after 13 steps.\n",
      "\n",
      "### Minimizing non-quadratic problem with solutions: x = -5 or x = 12.\n",
      "\n",
      "Applying minimization procedure with direction method: Steepest Descent\n",
      "Found the minimizer=[12.] with f(x)=-2520.0 after 16 steps.\n",
      "\n",
      "Applying minimization procedure with direction method: Newton\n",
      "Found the minimizer=[-5.] with f(x)=-472.91666666666663 after 8 steps.\n",
      "\n",
      "Applying minimization procedure with direction method: BFGS Quasi-Newton\n",
      "Found the minimizer=[-5.] with f(x)=-472.91666666666663 after 10 steps.\n",
      "\n",
      "Applying minimization procedure with direction method: Fletcher-Reeves conjugate\n",
      "Found the minimizer=[12.] with f(x)=-2520.0 after 74 steps.\n",
      "\n",
      "### Minimizing non-quadratic problem with solutions: x = -2 or x = 10.\n",
      "\n",
      "Applying minimization procedure with direction method: Steepest Descent\n",
      "Found the minimizer=[10.] with f(x)=-633.3333333333333 after 19 steps.\n",
      "\n",
      "Applying minimization procedure with direction method: Newton\n",
      "Found the minimizer=[-2.] with f(x)=-57.33333333333333 after 9 steps.\n",
      "\n",
      "Applying minimization procedure with direction method: BFGS Quasi-Newton\n",
      "Found the minimizer=[-2.] with f(x)=-57.33333333333333 after 11 steps.\n",
      "\n",
      "Applying minimization procedure with direction method: Fletcher-Reeves conjugate\n",
      "Found the minimizer=[10.] with f(x)=-633.3333333333333 after 16 steps.\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(41) # reproducability\n",
    "\n",
    "problem_count = 5\n",
    "\n",
    "print(\"Starting minimization procedure for non-quadratic problems (polynomials of 4th degree) ...\")\n",
    "\n",
    "for i in range(problem_count):\n",
    "    problem = create_non_quadratic_problem()\n",
    "    x0 = np.array([-20], dtype=np.float64) # choose x0 s.t. we can assume global convergence for newton\n",
    "\n",
    "    print(f\"\\n### Minimizing non-quadratic problem with solutions: x = {problem.solution[0]} or x = {problem.solution[1]}.\")\n",
    "\n",
    "    for name, direction_method in direction_methods.items():\n",
    "        print(f\"\\nApplying minimization procedure with direction method: {name}\")\n",
    "        x_minimizer, grad_norms = alg.find_minimizer(problem, x0, direction_method, tolerance=1e-6, max_iter=1000)\n",
    "        x_minimizer = np.around(x_minimizer, 3) # assuming an integer solution in our case (gets rid of nasty precision errors - mostly from FR method)\n",
    "        print(f\"Found the minimizer={x_minimizer} with f(x)={problem.f(x_minimizer)} after {len(grad_norms)} steps.\")"
   ]
  },
  {
   "source": [
    "# Show code from problems.py\n",
    "\n",
    "filename = \"src\\\\problems.py\"\n",
    "with open(filename, 'r') as filehandle:\n",
    "    filecontent = filehandle.read()\n",
    "\n",
    "Code(filecontent)"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": 11,
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "import numpy as np\n",
       "from scipy.stats import ortho_group\n",
       "\n",
       "from src.algorithms import MinimizationProblem\n",
       "\n",
       "# original function\n",
       "def rosenbrock(x):\n",
       "    return 100*(x[1]-x[0]**2)**2 + (1-x[0])**2\n",
       "\n",
       "# first derivative\n",
       "def d_rosenbrock(x):\n",
       "    dx = 400*x[0]**3-400*x[0]*x[1]+2*x[0]-2\n",
       "    dy = 200*x[1]-200*x[0]**2\n",
       "\n",
       "    return np.array([dx, dy])\n",
       "\n",
       "# second derivative\n",
       "def d2_rosenbrock(x):\n",
       "    ddx = 1200*x[0]**2-400*x[1]+2\n",
       "    ddxy = -400*x[0]\n",
       "    ddy = 200\n",
       "\n",
       "    return np.array([[ddx, ddxy], [ddxy, ddy]])\n",
       "\n",
       "ROSENBROCK_PROBLEM = MinimizationProblem(rosenbrock, d_rosenbrock, d2_rosenbrock, np.array([1, 1]))\n",
       "\n",
       "def create_quadratic_problem(n, random_state=None):\n",
       "    if random_state is not None:\n",
       "        np.random.seed(random_state)\n",
       "\n",
       "    A_generator = np.random.randint(low=1, high=10, size=(n, n))\n",
       "\n",
       "    A = A_generator @ A_generator.T # -> positive definite matrix\n",
       "    solution = np.random.randint(low=1, high=10, size=(n))\n",
       "    b = A @ solution\n",
       "\n",
       "    def f(x):\n",
       "        return 1/2 * x @ A @ x - b @ x\n",
       "    \n",
       "    def d_f(x):\n",
       "        return A @ x - b\n",
       "    \n",
       "    def d2_f(x):\n",
       "        return A\n",
       "\n",
       "    return MinimizationProblem(f, d_f, d2_f, solution)\n",
       "\n",
       "def create_non_quadratic_problem(random_state=None):\n",
       "    if random_state is not None:\n",
       "        np.random.seed(random_state)\n",
       "\n",
       "    a = np.random.randint(low=-10, high=-1)\n",
       "    b = np.random.randint(low=-1, high=7)\n",
       "    c = np.random.randint(low=7, high=15)\n",
       "\n",
       "    # 2 minimizers possible\n",
       "    solution = [a, c]\n",
       "\n",
       "    # integral of (x-a)(x-b)(x-c)\n",
       "    def f(x):\n",
       "        return 1/12 * x[0]*(-4*x[0]**2* (a+b+c) + 6*x[0]*(a*(b+c) + b*c) - 12*a*b*c + 3*x[0]**3)\n",
       "    \n",
       "    def d_f(x):\n",
       "        return (x-a)*(x-b)*(x-c)\n",
       "    \n",
       "    def d2_f(x):\n",
       "        return np.array([a*(b+c-2*x) + b*(c-2*x) + x*(3*x-2*c)])\n",
       "\n",
       "    return MinimizationProblem(f, d_f, d2_f, solution)"
      ],
      "text/html": "<style>pre { line-height: 125%; }\ntd.linenos .normal { color: inherit; background-color: transparent; padding-left: 5px; padding-right: 5px; }\nspan.linenos { color: inherit; background-color: transparent; padding-left: 5px; padding-right: 5px; }\ntd.linenos .special { color: #000000; background-color: #ffffc0; padding-left: 5px; padding-right: 5px; }\nspan.linenos.special { color: #000000; background-color: #ffffc0; padding-left: 5px; padding-right: 5px; }\n.output_html .hll { background-color: #ffffcc }\n.output_html { background: #f8f8f8; }\n.output_html .c { color: #408080; font-style: italic } /* Comment */\n.output_html .err { border: 1px solid #FF0000 } /* Error */\n.output_html .k { color: #008000; font-weight: bold } /* Keyword */\n.output_html .o { color: #666666 } /* Operator */\n.output_html .ch { color: #408080; font-style: italic } /* Comment.Hashbang */\n.output_html .cm { color: #408080; font-style: italic } /* Comment.Multiline */\n.output_html .cp { color: #BC7A00 } /* Comment.Preproc */\n.output_html .cpf { color: #408080; font-style: italic } /* Comment.PreprocFile */\n.output_html .c1 { color: #408080; font-style: italic } /* Comment.Single */\n.output_html .cs { color: #408080; font-style: italic } /* Comment.Special */\n.output_html .gd { color: #A00000 } /* Generic.Deleted */\n.output_html .ge { font-style: italic } /* Generic.Emph */\n.output_html .gr { color: #FF0000 } /* Generic.Error */\n.output_html .gh { color: #000080; font-weight: bold } /* Generic.Heading */\n.output_html .gi { color: #00A000 } /* Generic.Inserted */\n.output_html .go { color: #888888 } /* Generic.Output */\n.output_html .gp { color: #000080; font-weight: bold } /* Generic.Prompt */\n.output_html .gs { font-weight: bold } /* Generic.Strong */\n.output_html .gu { color: #800080; font-weight: bold } /* Generic.Subheading */\n.output_html .gt { color: #0044DD } /* Generic.Traceback */\n.output_html .kc { color: #008000; font-weight: bold } /* Keyword.Constant */\n.output_html .kd { color: #008000; font-weight: bold } /* Keyword.Declaration */\n.output_html .kn { color: #008000; font-weight: bold } /* Keyword.Namespace */\n.output_html .kp { color: #008000 } /* Keyword.Pseudo */\n.output_html .kr { color: #008000; font-weight: bold } /* Keyword.Reserved */\n.output_html .kt { color: #B00040 } /* Keyword.Type */\n.output_html .m { color: #666666 } /* Literal.Number */\n.output_html .s { color: #BA2121 } /* Literal.String */\n.output_html .na { color: #7D9029 } /* Name.Attribute */\n.output_html .nb { color: #008000 } /* Name.Builtin */\n.output_html .nc { color: #0000FF; font-weight: bold } /* Name.Class */\n.output_html .no { color: #880000 } /* Name.Constant */\n.output_html .nd { color: #AA22FF } /* Name.Decorator */\n.output_html .ni { color: #999999; font-weight: bold } /* Name.Entity */\n.output_html .ne { color: #D2413A; font-weight: bold } /* Name.Exception */\n.output_html .nf { color: #0000FF } /* Name.Function */\n.output_html .nl { color: #A0A000 } /* Name.Label */\n.output_html .nn { color: #0000FF; font-weight: bold } /* Name.Namespace */\n.output_html .nt { color: #008000; font-weight: bold } /* Name.Tag */\n.output_html .nv { color: #19177C } /* Name.Variable */\n.output_html .ow { color: #AA22FF; font-weight: bold } /* Operator.Word */\n.output_html .w { color: #bbbbbb } /* Text.Whitespace */\n.output_html .mb { color: #666666 } /* Literal.Number.Bin */\n.output_html .mf { color: #666666 } /* Literal.Number.Float */\n.output_html .mh { color: #666666 } /* Literal.Number.Hex */\n.output_html .mi { color: #666666 } /* Literal.Number.Integer */\n.output_html .mo { color: #666666 } /* Literal.Number.Oct */\n.output_html .sa { color: #BA2121 } /* Literal.String.Affix */\n.output_html .sb { color: #BA2121 } /* Literal.String.Backtick */\n.output_html .sc { color: #BA2121 } /* Literal.String.Char */\n.output_html .dl { color: #BA2121 } /* Literal.String.Delimiter */\n.output_html .sd { color: #BA2121; font-style: italic } /* Literal.String.Doc */\n.output_html .s2 { color: #BA2121 } /* Literal.String.Double */\n.output_html .se { color: #BB6622; font-weight: bold } /* Literal.String.Escape */\n.output_html .sh { color: #BA2121 } /* Literal.String.Heredoc */\n.output_html .si { color: #BB6688; font-weight: bold } /* Literal.String.Interpol */\n.output_html .sx { color: #008000 } /* Literal.String.Other */\n.output_html .sr { color: #BB6688 } /* Literal.String.Regex */\n.output_html .s1 { color: #BA2121 } /* Literal.String.Single */\n.output_html .ss { color: #19177C } /* Literal.String.Symbol */\n.output_html .bp { color: #008000 } /* Name.Builtin.Pseudo */\n.output_html .fm { color: #0000FF } /* Name.Function.Magic */\n.output_html .vc { color: #19177C } /* Name.Variable.Class */\n.output_html .vg { color: #19177C } /* Name.Variable.Global */\n.output_html .vi { color: #19177C } /* Name.Variable.Instance */\n.output_html .vm { color: #19177C } /* Name.Variable.Magic */\n.output_html .il { color: #666666 } /* Literal.Number.Integer.Long */</style><div class=\"highlight\"><pre><span></span><span class=\"kn\">import</span> <span class=\"nn\">numpy</span> <span class=\"k\">as</span> <span class=\"nn\">np</span>\n<span class=\"kn\">from</span> <span class=\"nn\">scipy.stats</span> <span class=\"kn\">import</span> <span class=\"n\">ortho_group</span>\n\n<span class=\"kn\">from</span> <span class=\"nn\">src.algorithms</span> <span class=\"kn\">import</span> <span class=\"n\">MinimizationProblem</span>\n\n<span class=\"c1\"># original function</span>\n<span class=\"k\">def</span> <span class=\"nf\">rosenbrock</span><span class=\"p\">(</span><span class=\"n\">x</span><span class=\"p\">):</span>\n    <span class=\"k\">return</span> <span class=\"mi\">100</span><span class=\"o\">*</span><span class=\"p\">(</span><span class=\"n\">x</span><span class=\"p\">[</span><span class=\"mi\">1</span><span class=\"p\">]</span><span class=\"o\">-</span><span class=\"n\">x</span><span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">]</span><span class=\"o\">**</span><span class=\"mi\">2</span><span class=\"p\">)</span><span class=\"o\">**</span><span class=\"mi\">2</span> <span class=\"o\">+</span> <span class=\"p\">(</span><span class=\"mi\">1</span><span class=\"o\">-</span><span class=\"n\">x</span><span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">])</span><span class=\"o\">**</span><span class=\"mi\">2</span>\n\n<span class=\"c1\"># first derivative</span>\n<span class=\"k\">def</span> <span class=\"nf\">d_rosenbrock</span><span class=\"p\">(</span><span class=\"n\">x</span><span class=\"p\">):</span>\n    <span class=\"n\">dx</span> <span class=\"o\">=</span> <span class=\"mi\">400</span><span class=\"o\">*</span><span class=\"n\">x</span><span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">]</span><span class=\"o\">**</span><span class=\"mi\">3</span><span class=\"o\">-</span><span class=\"mi\">400</span><span class=\"o\">*</span><span class=\"n\">x</span><span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">]</span><span class=\"o\">*</span><span class=\"n\">x</span><span class=\"p\">[</span><span class=\"mi\">1</span><span class=\"p\">]</span><span class=\"o\">+</span><span class=\"mi\">2</span><span class=\"o\">*</span><span class=\"n\">x</span><span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">]</span><span class=\"o\">-</span><span class=\"mi\">2</span>\n    <span class=\"n\">dy</span> <span class=\"o\">=</span> <span class=\"mi\">200</span><span class=\"o\">*</span><span class=\"n\">x</span><span class=\"p\">[</span><span class=\"mi\">1</span><span class=\"p\">]</span><span class=\"o\">-</span><span class=\"mi\">200</span><span class=\"o\">*</span><span class=\"n\">x</span><span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">]</span><span class=\"o\">**</span><span class=\"mi\">2</span>\n\n    <span class=\"k\">return</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"kp\">array</span><span class=\"p\">([</span><span class=\"n\">dx</span><span class=\"p\">,</span> <span class=\"n\">dy</span><span class=\"p\">])</span>\n\n<span class=\"c1\"># second derivative</span>\n<span class=\"k\">def</span> <span class=\"nf\">d2_rosenbrock</span><span class=\"p\">(</span><span class=\"n\">x</span><span class=\"p\">):</span>\n    <span class=\"n\">ddx</span> <span class=\"o\">=</span> <span class=\"mi\">1200</span><span class=\"o\">*</span><span class=\"n\">x</span><span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">]</span><span class=\"o\">**</span><span class=\"mi\">2</span><span class=\"o\">-</span><span class=\"mi\">400</span><span class=\"o\">*</span><span class=\"n\">x</span><span class=\"p\">[</span><span class=\"mi\">1</span><span class=\"p\">]</span><span class=\"o\">+</span><span class=\"mi\">2</span>\n    <span class=\"n\">ddxy</span> <span class=\"o\">=</span> <span class=\"o\">-</span><span class=\"mi\">400</span><span class=\"o\">*</span><span class=\"n\">x</span><span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">]</span>\n    <span class=\"n\">ddy</span> <span class=\"o\">=</span> <span class=\"mi\">200</span>\n\n    <span class=\"k\">return</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"kp\">array</span><span class=\"p\">([[</span><span class=\"n\">ddx</span><span class=\"p\">,</span> <span class=\"n\">ddxy</span><span class=\"p\">],</span> <span class=\"p\">[</span><span class=\"n\">ddxy</span><span class=\"p\">,</span> <span class=\"n\">ddy</span><span class=\"p\">]])</span>\n\n<span class=\"n\">ROSENBROCK_PROBLEM</span> <span class=\"o\">=</span> <span class=\"n\">MinimizationProblem</span><span class=\"p\">(</span><span class=\"n\">rosenbrock</span><span class=\"p\">,</span> <span class=\"n\">d_rosenbrock</span><span class=\"p\">,</span> <span class=\"n\">d2_rosenbrock</span><span class=\"p\">,</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"kp\">array</span><span class=\"p\">([</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"p\">]))</span>\n\n<span class=\"k\">def</span> <span class=\"nf\">create_quadratic_problem</span><span class=\"p\">(</span><span class=\"n\">n</span><span class=\"p\">,</span> <span class=\"n\">random_state</span><span class=\"o\">=</span><span class=\"kc\">None</span><span class=\"p\">):</span>\n    <span class=\"k\">if</span> <span class=\"n\">random_state</span> <span class=\"ow\">is</span> <span class=\"ow\">not</span> <span class=\"kc\">None</span><span class=\"p\">:</span>\n        <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">random</span><span class=\"o\">.</span><span class=\"kp\">seed</span><span class=\"p\">(</span><span class=\"n\">random_state</span><span class=\"p\">)</span>\n\n    <span class=\"n\">A_generator</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">random</span><span class=\"o\">.</span><span class=\"kp\">randint</span><span class=\"p\">(</span><span class=\"n\">low</span><span class=\"o\">=</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"n\">high</span><span class=\"o\">=</span><span class=\"mi\">10</span><span class=\"p\">,</span> <span class=\"kp\">size</span><span class=\"o\">=</span><span class=\"p\">(</span><span class=\"n\">n</span><span class=\"p\">,</span> <span class=\"n\">n</span><span class=\"p\">))</span>\n\n    <span class=\"n\">A</span> <span class=\"o\">=</span> <span class=\"n\">A_generator</span> <span class=\"o\">@</span> <span class=\"n\">A_generator</span><span class=\"o\">.</span><span class=\"n\">T</span> <span class=\"c1\"># -&gt; positive definite matrix</span>\n    <span class=\"n\">solution</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">random</span><span class=\"o\">.</span><span class=\"kp\">randint</span><span class=\"p\">(</span><span class=\"n\">low</span><span class=\"o\">=</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"n\">high</span><span class=\"o\">=</span><span class=\"mi\">10</span><span class=\"p\">,</span> <span class=\"kp\">size</span><span class=\"o\">=</span><span class=\"p\">(</span><span class=\"n\">n</span><span class=\"p\">))</span>\n    <span class=\"n\">b</span> <span class=\"o\">=</span> <span class=\"n\">A</span> <span class=\"o\">@</span> <span class=\"n\">solution</span>\n\n    <span class=\"k\">def</span> <span class=\"nf\">f</span><span class=\"p\">(</span><span class=\"n\">x</span><span class=\"p\">):</span>\n        <span class=\"k\">return</span> <span class=\"mi\">1</span><span class=\"o\">/</span><span class=\"mi\">2</span> <span class=\"o\">*</span> <span class=\"n\">x</span> <span class=\"o\">@</span> <span class=\"n\">A</span> <span class=\"o\">@</span> <span class=\"n\">x</span> <span class=\"o\">-</span> <span class=\"n\">b</span> <span class=\"o\">@</span> <span class=\"n\">x</span>\n    \n    <span class=\"k\">def</span> <span class=\"nf\">d_f</span><span class=\"p\">(</span><span class=\"n\">x</span><span class=\"p\">):</span>\n        <span class=\"k\">return</span> <span class=\"n\">A</span> <span class=\"o\">@</span> <span class=\"n\">x</span> <span class=\"o\">-</span> <span class=\"n\">b</span>\n    \n    <span class=\"k\">def</span> <span class=\"nf\">d2_f</span><span class=\"p\">(</span><span class=\"n\">x</span><span class=\"p\">):</span>\n        <span class=\"k\">return</span> <span class=\"n\">A</span>\n\n    <span class=\"k\">return</span> <span class=\"n\">MinimizationProblem</span><span class=\"p\">(</span><span class=\"n\">f</span><span class=\"p\">,</span> <span class=\"n\">d_f</span><span class=\"p\">,</span> <span class=\"n\">d2_f</span><span class=\"p\">,</span> <span class=\"n\">solution</span><span class=\"p\">)</span>\n\n<span class=\"k\">def</span> <span class=\"nf\">create_non_quadratic_problem</span><span class=\"p\">(</span><span class=\"n\">random_state</span><span class=\"o\">=</span><span class=\"kc\">None</span><span class=\"p\">):</span>\n    <span class=\"k\">if</span> <span class=\"n\">random_state</span> <span class=\"ow\">is</span> <span class=\"ow\">not</span> <span class=\"kc\">None</span><span class=\"p\">:</span>\n        <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">random</span><span class=\"o\">.</span><span class=\"kp\">seed</span><span class=\"p\">(</span><span class=\"n\">random_state</span><span class=\"p\">)</span>\n\n    <span class=\"n\">a</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">random</span><span class=\"o\">.</span><span class=\"kp\">randint</span><span class=\"p\">(</span><span class=\"n\">low</span><span class=\"o\">=-</span><span class=\"mi\">10</span><span class=\"p\">,</span> <span class=\"n\">high</span><span class=\"o\">=-</span><span class=\"mi\">1</span><span class=\"p\">)</span>\n    <span class=\"n\">b</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">random</span><span class=\"o\">.</span><span class=\"kp\">randint</span><span class=\"p\">(</span><span class=\"n\">low</span><span class=\"o\">=-</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"n\">high</span><span class=\"o\">=</span><span class=\"mi\">7</span><span class=\"p\">)</span>\n    <span class=\"n\">c</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">random</span><span class=\"o\">.</span><span class=\"kp\">randint</span><span class=\"p\">(</span><span class=\"n\">low</span><span class=\"o\">=</span><span class=\"mi\">7</span><span class=\"p\">,</span> <span class=\"n\">high</span><span class=\"o\">=</span><span class=\"mi\">15</span><span class=\"p\">)</span>\n\n    <span class=\"c1\"># 2 minimizers possible</span>\n    <span class=\"n\">solution</span> <span class=\"o\">=</span> <span class=\"p\">[</span><span class=\"n\">a</span><span class=\"p\">,</span> <span class=\"n\">c</span><span class=\"p\">]</span>\n\n    <span class=\"c1\"># integral of (x-a)(x-b)(x-c)</span>\n    <span class=\"k\">def</span> <span class=\"nf\">f</span><span class=\"p\">(</span><span class=\"n\">x</span><span class=\"p\">):</span>\n        <span class=\"k\">return</span> <span class=\"mi\">1</span><span class=\"o\">/</span><span class=\"mi\">12</span> <span class=\"o\">*</span> <span class=\"n\">x</span><span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">]</span><span class=\"o\">*</span><span class=\"p\">(</span><span class=\"o\">-</span><span class=\"mi\">4</span><span class=\"o\">*</span><span class=\"n\">x</span><span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">]</span><span class=\"o\">**</span><span class=\"mi\">2</span><span class=\"o\">*</span> <span class=\"p\">(</span><span class=\"n\">a</span><span class=\"o\">+</span><span class=\"n\">b</span><span class=\"o\">+</span><span class=\"n\">c</span><span class=\"p\">)</span> <span class=\"o\">+</span> <span class=\"mi\">6</span><span class=\"o\">*</span><span class=\"n\">x</span><span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">]</span><span class=\"o\">*</span><span class=\"p\">(</span><span class=\"n\">a</span><span class=\"o\">*</span><span class=\"p\">(</span><span class=\"n\">b</span><span class=\"o\">+</span><span class=\"n\">c</span><span class=\"p\">)</span> <span class=\"o\">+</span> <span class=\"n\">b</span><span class=\"o\">*</span><span class=\"n\">c</span><span class=\"p\">)</span> <span class=\"o\">-</span> <span class=\"mi\">12</span><span class=\"o\">*</span><span class=\"n\">a</span><span class=\"o\">*</span><span class=\"n\">b</span><span class=\"o\">*</span><span class=\"n\">c</span> <span class=\"o\">+</span> <span class=\"mi\">3</span><span class=\"o\">*</span><span class=\"n\">x</span><span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">]</span><span class=\"o\">**</span><span class=\"mi\">3</span><span class=\"p\">)</span>\n    \n    <span class=\"k\">def</span> <span class=\"nf\">d_f</span><span class=\"p\">(</span><span class=\"n\">x</span><span class=\"p\">):</span>\n        <span class=\"k\">return</span> <span class=\"p\">(</span><span class=\"n\">x</span><span class=\"o\">-</span><span class=\"n\">a</span><span class=\"p\">)</span><span class=\"o\">*</span><span class=\"p\">(</span><span class=\"n\">x</span><span class=\"o\">-</span><span class=\"n\">b</span><span class=\"p\">)</span><span class=\"o\">*</span><span class=\"p\">(</span><span class=\"n\">x</span><span class=\"o\">-</span><span class=\"n\">c</span><span class=\"p\">)</span>\n    \n    <span class=\"k\">def</span> <span class=\"nf\">d2_f</span><span class=\"p\">(</span><span class=\"n\">x</span><span class=\"p\">):</span>\n        <span class=\"k\">return</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"kp\">array</span><span class=\"p\">([</span><span class=\"n\">a</span><span class=\"o\">*</span><span class=\"p\">(</span><span class=\"n\">b</span><span class=\"o\">+</span><span class=\"n\">c</span><span class=\"o\">-</span><span class=\"mi\">2</span><span class=\"o\">*</span><span class=\"n\">x</span><span class=\"p\">)</span> <span class=\"o\">+</span> <span class=\"n\">b</span><span class=\"o\">*</span><span class=\"p\">(</span><span class=\"n\">c</span><span class=\"o\">-</span><span class=\"mi\">2</span><span class=\"o\">*</span><span class=\"n\">x</span><span class=\"p\">)</span> <span class=\"o\">+</span> <span class=\"n\">x</span><span class=\"o\">*</span><span class=\"p\">(</span><span class=\"mi\">3</span><span class=\"o\">*</span><span class=\"n\">x</span><span class=\"o\">-</span><span class=\"mi\">2</span><span class=\"o\">*</span><span class=\"n\">c</span><span class=\"p\">)])</span>\n\n    <span class=\"k\">return</span> <span class=\"n\">MinimizationProblem</span><span class=\"p\">(</span><span class=\"n\">f</span><span class=\"p\">,</span> <span class=\"n\">d_f</span><span class=\"p\">,</span> <span class=\"n\">d2_f</span><span class=\"p\">,</span> <span class=\"n\">solution</span><span class=\"p\">)</span>\n</pre></div>\n",
      "text/latex": "\\begin{Verbatim}[commandchars=\\\\\\{\\}]\n\\PY{k+kn}{import} \\PY{n+nn}{numpy} \\PY{k}{as} \\PY{n+nn}{np}\n\\PY{k+kn}{from} \\PY{n+nn}{scipy}\\PY{n+nn}{.}\\PY{n+nn}{stats} \\PY{k+kn}{import} \\PY{n}{ortho\\PYZus{}group}\n\n\\PY{k+kn}{from} \\PY{n+nn}{src}\\PY{n+nn}{.}\\PY{n+nn}{algorithms} \\PY{k+kn}{import} \\PY{n}{MinimizationProblem}\n\n\\PY{c+c1}{\\PYZsh{} original function}\n\\PY{k}{def} \\PY{n+nf}{rosenbrock}\\PY{p}{(}\\PY{n}{x}\\PY{p}{)}\\PY{p}{:}\n    \\PY{k}{return} \\PY{l+m+mi}{100}\\PY{o}{*}\\PY{p}{(}\\PY{n}{x}\\PY{p}{[}\\PY{l+m+mi}{1}\\PY{p}{]}\\PY{o}{\\PYZhy{}}\\PY{n}{x}\\PY{p}{[}\\PY{l+m+mi}{0}\\PY{p}{]}\\PY{o}{*}\\PY{o}{*}\\PY{l+m+mi}{2}\\PY{p}{)}\\PY{o}{*}\\PY{o}{*}\\PY{l+m+mi}{2} \\PY{o}{+} \\PY{p}{(}\\PY{l+m+mi}{1}\\PY{o}{\\PYZhy{}}\\PY{n}{x}\\PY{p}{[}\\PY{l+m+mi}{0}\\PY{p}{]}\\PY{p}{)}\\PY{o}{*}\\PY{o}{*}\\PY{l+m+mi}{2}\n\n\\PY{c+c1}{\\PYZsh{} first derivative}\n\\PY{k}{def} \\PY{n+nf}{d\\PYZus{}rosenbrock}\\PY{p}{(}\\PY{n}{x}\\PY{p}{)}\\PY{p}{:}\n    \\PY{n}{dx} \\PY{o}{=} \\PY{l+m+mi}{400}\\PY{o}{*}\\PY{n}{x}\\PY{p}{[}\\PY{l+m+mi}{0}\\PY{p}{]}\\PY{o}{*}\\PY{o}{*}\\PY{l+m+mi}{3}\\PY{o}{\\PYZhy{}}\\PY{l+m+mi}{400}\\PY{o}{*}\\PY{n}{x}\\PY{p}{[}\\PY{l+m+mi}{0}\\PY{p}{]}\\PY{o}{*}\\PY{n}{x}\\PY{p}{[}\\PY{l+m+mi}{1}\\PY{p}{]}\\PY{o}{+}\\PY{l+m+mi}{2}\\PY{o}{*}\\PY{n}{x}\\PY{p}{[}\\PY{l+m+mi}{0}\\PY{p}{]}\\PY{o}{\\PYZhy{}}\\PY{l+m+mi}{2}\n    \\PY{n}{dy} \\PY{o}{=} \\PY{l+m+mi}{200}\\PY{o}{*}\\PY{n}{x}\\PY{p}{[}\\PY{l+m+mi}{1}\\PY{p}{]}\\PY{o}{\\PYZhy{}}\\PY{l+m+mi}{200}\\PY{o}{*}\\PY{n}{x}\\PY{p}{[}\\PY{l+m+mi}{0}\\PY{p}{]}\\PY{o}{*}\\PY{o}{*}\\PY{l+m+mi}{2}\n\n    \\PY{k}{return} \\PY{n}{np}\\PY{o}{.}\\PY{k+kp}{array}\\PY{p}{(}\\PY{p}{[}\\PY{n}{dx}\\PY{p}{,} \\PY{n}{dy}\\PY{p}{]}\\PY{p}{)}\n\n\\PY{c+c1}{\\PYZsh{} second derivative}\n\\PY{k}{def} \\PY{n+nf}{d2\\PYZus{}rosenbrock}\\PY{p}{(}\\PY{n}{x}\\PY{p}{)}\\PY{p}{:}\n    \\PY{n}{ddx} \\PY{o}{=} \\PY{l+m+mi}{1200}\\PY{o}{*}\\PY{n}{x}\\PY{p}{[}\\PY{l+m+mi}{0}\\PY{p}{]}\\PY{o}{*}\\PY{o}{*}\\PY{l+m+mi}{2}\\PY{o}{\\PYZhy{}}\\PY{l+m+mi}{400}\\PY{o}{*}\\PY{n}{x}\\PY{p}{[}\\PY{l+m+mi}{1}\\PY{p}{]}\\PY{o}{+}\\PY{l+m+mi}{2}\n    \\PY{n}{ddxy} \\PY{o}{=} \\PY{o}{\\PYZhy{}}\\PY{l+m+mi}{400}\\PY{o}{*}\\PY{n}{x}\\PY{p}{[}\\PY{l+m+mi}{0}\\PY{p}{]}\n    \\PY{n}{ddy} \\PY{o}{=} \\PY{l+m+mi}{200}\n\n    \\PY{k}{return} \\PY{n}{np}\\PY{o}{.}\\PY{k+kp}{array}\\PY{p}{(}\\PY{p}{[}\\PY{p}{[}\\PY{n}{ddx}\\PY{p}{,} \\PY{n}{ddxy}\\PY{p}{]}\\PY{p}{,} \\PY{p}{[}\\PY{n}{ddxy}\\PY{p}{,} \\PY{n}{ddy}\\PY{p}{]}\\PY{p}{]}\\PY{p}{)}\n\n\\PY{n}{ROSENBROCK\\PYZus{}PROBLEM} \\PY{o}{=} \\PY{n}{MinimizationProblem}\\PY{p}{(}\\PY{n}{rosenbrock}\\PY{p}{,} \\PY{n}{d\\PYZus{}rosenbrock}\\PY{p}{,} \\PY{n}{d2\\PYZus{}rosenbrock}\\PY{p}{,} \\PY{n}{np}\\PY{o}{.}\\PY{k+kp}{array}\\PY{p}{(}\\PY{p}{[}\\PY{l+m+mi}{1}\\PY{p}{,} \\PY{l+m+mi}{1}\\PY{p}{]}\\PY{p}{)}\\PY{p}{)}\n\n\\PY{k}{def} \\PY{n+nf}{create\\PYZus{}quadratic\\PYZus{}problem}\\PY{p}{(}\\PY{n}{n}\\PY{p}{,} \\PY{n}{random\\PYZus{}state}\\PY{o}{=}\\PY{k+kc}{None}\\PY{p}{)}\\PY{p}{:}\n    \\PY{k}{if} \\PY{n}{random\\PYZus{}state} \\PY{o+ow}{is} \\PY{o+ow}{not} \\PY{k+kc}{None}\\PY{p}{:}\n        \\PY{n}{np}\\PY{o}{.}\\PY{n}{random}\\PY{o}{.}\\PY{k+kp}{seed}\\PY{p}{(}\\PY{n}{random\\PYZus{}state}\\PY{p}{)}\n\n    \\PY{n}{A\\PYZus{}generator} \\PY{o}{=} \\PY{n}{np}\\PY{o}{.}\\PY{n}{random}\\PY{o}{.}\\PY{k+kp}{randint}\\PY{p}{(}\\PY{n}{low}\\PY{o}{=}\\PY{l+m+mi}{1}\\PY{p}{,} \\PY{n}{high}\\PY{o}{=}\\PY{l+m+mi}{10}\\PY{p}{,} \\PY{k+kp}{size}\\PY{o}{=}\\PY{p}{(}\\PY{n}{n}\\PY{p}{,} \\PY{n}{n}\\PY{p}{)}\\PY{p}{)}\n\n    \\PY{n}{A} \\PY{o}{=} \\PY{n}{A\\PYZus{}generator} \\PY{o}{@} \\PY{n}{A\\PYZus{}generator}\\PY{o}{.}\\PY{n}{T} \\PY{c+c1}{\\PYZsh{} \\PYZhy{}\\PYZgt{} positive definite matrix}\n    \\PY{n}{solution} \\PY{o}{=} \\PY{n}{np}\\PY{o}{.}\\PY{n}{random}\\PY{o}{.}\\PY{k+kp}{randint}\\PY{p}{(}\\PY{n}{low}\\PY{o}{=}\\PY{l+m+mi}{1}\\PY{p}{,} \\PY{n}{high}\\PY{o}{=}\\PY{l+m+mi}{10}\\PY{p}{,} \\PY{k+kp}{size}\\PY{o}{=}\\PY{p}{(}\\PY{n}{n}\\PY{p}{)}\\PY{p}{)}\n    \\PY{n}{b} \\PY{o}{=} \\PY{n}{A} \\PY{o}{@} \\PY{n}{solution}\n\n    \\PY{k}{def} \\PY{n+nf}{f}\\PY{p}{(}\\PY{n}{x}\\PY{p}{)}\\PY{p}{:}\n        \\PY{k}{return} \\PY{l+m+mi}{1}\\PY{o}{/}\\PY{l+m+mi}{2} \\PY{o}{*} \\PY{n}{x} \\PY{o}{@} \\PY{n}{A} \\PY{o}{@} \\PY{n}{x} \\PY{o}{\\PYZhy{}} \\PY{n}{b} \\PY{o}{@} \\PY{n}{x}\n    \n    \\PY{k}{def} \\PY{n+nf}{d\\PYZus{}f}\\PY{p}{(}\\PY{n}{x}\\PY{p}{)}\\PY{p}{:}\n        \\PY{k}{return} \\PY{n}{A} \\PY{o}{@} \\PY{n}{x} \\PY{o}{\\PYZhy{}} \\PY{n}{b}\n    \n    \\PY{k}{def} \\PY{n+nf}{d2\\PYZus{}f}\\PY{p}{(}\\PY{n}{x}\\PY{p}{)}\\PY{p}{:}\n        \\PY{k}{return} \\PY{n}{A}\n\n    \\PY{k}{return} \\PY{n}{MinimizationProblem}\\PY{p}{(}\\PY{n}{f}\\PY{p}{,} \\PY{n}{d\\PYZus{}f}\\PY{p}{,} \\PY{n}{d2\\PYZus{}f}\\PY{p}{,} \\PY{n}{solution}\\PY{p}{)}\n\n\\PY{k}{def} \\PY{n+nf}{create\\PYZus{}non\\PYZus{}quadratic\\PYZus{}problem}\\PY{p}{(}\\PY{n}{random\\PYZus{}state}\\PY{o}{=}\\PY{k+kc}{None}\\PY{p}{)}\\PY{p}{:}\n    \\PY{k}{if} \\PY{n}{random\\PYZus{}state} \\PY{o+ow}{is} \\PY{o+ow}{not} \\PY{k+kc}{None}\\PY{p}{:}\n        \\PY{n}{np}\\PY{o}{.}\\PY{n}{random}\\PY{o}{.}\\PY{k+kp}{seed}\\PY{p}{(}\\PY{n}{random\\PYZus{}state}\\PY{p}{)}\n\n    \\PY{n}{a} \\PY{o}{=} \\PY{n}{np}\\PY{o}{.}\\PY{n}{random}\\PY{o}{.}\\PY{k+kp}{randint}\\PY{p}{(}\\PY{n}{low}\\PY{o}{=}\\PY{o}{\\PYZhy{}}\\PY{l+m+mi}{10}\\PY{p}{,} \\PY{n}{high}\\PY{o}{=}\\PY{o}{\\PYZhy{}}\\PY{l+m+mi}{1}\\PY{p}{)}\n    \\PY{n}{b} \\PY{o}{=} \\PY{n}{np}\\PY{o}{.}\\PY{n}{random}\\PY{o}{.}\\PY{k+kp}{randint}\\PY{p}{(}\\PY{n}{low}\\PY{o}{=}\\PY{o}{\\PYZhy{}}\\PY{l+m+mi}{1}\\PY{p}{,} \\PY{n}{high}\\PY{o}{=}\\PY{l+m+mi}{7}\\PY{p}{)}\n    \\PY{n}{c} \\PY{o}{=} \\PY{n}{np}\\PY{o}{.}\\PY{n}{random}\\PY{o}{.}\\PY{k+kp}{randint}\\PY{p}{(}\\PY{n}{low}\\PY{o}{=}\\PY{l+m+mi}{7}\\PY{p}{,} \\PY{n}{high}\\PY{o}{=}\\PY{l+m+mi}{15}\\PY{p}{)}\n\n    \\PY{c+c1}{\\PYZsh{} 2 minimizers possible}\n    \\PY{n}{solution} \\PY{o}{=} \\PY{p}{[}\\PY{n}{a}\\PY{p}{,} \\PY{n}{c}\\PY{p}{]}\n\n    \\PY{c+c1}{\\PYZsh{} integral of (x\\PYZhy{}a)(x\\PYZhy{}b)(x\\PYZhy{}c)}\n    \\PY{k}{def} \\PY{n+nf}{f}\\PY{p}{(}\\PY{n}{x}\\PY{p}{)}\\PY{p}{:}\n        \\PY{k}{return} \\PY{l+m+mi}{1}\\PY{o}{/}\\PY{l+m+mi}{12} \\PY{o}{*} \\PY{n}{x}\\PY{p}{[}\\PY{l+m+mi}{0}\\PY{p}{]}\\PY{o}{*}\\PY{p}{(}\\PY{o}{\\PYZhy{}}\\PY{l+m+mi}{4}\\PY{o}{*}\\PY{n}{x}\\PY{p}{[}\\PY{l+m+mi}{0}\\PY{p}{]}\\PY{o}{*}\\PY{o}{*}\\PY{l+m+mi}{2}\\PY{o}{*} \\PY{p}{(}\\PY{n}{a}\\PY{o}{+}\\PY{n}{b}\\PY{o}{+}\\PY{n}{c}\\PY{p}{)} \\PY{o}{+} \\PY{l+m+mi}{6}\\PY{o}{*}\\PY{n}{x}\\PY{p}{[}\\PY{l+m+mi}{0}\\PY{p}{]}\\PY{o}{*}\\PY{p}{(}\\PY{n}{a}\\PY{o}{*}\\PY{p}{(}\\PY{n}{b}\\PY{o}{+}\\PY{n}{c}\\PY{p}{)} \\PY{o}{+} \\PY{n}{b}\\PY{o}{*}\\PY{n}{c}\\PY{p}{)} \\PY{o}{\\PYZhy{}} \\PY{l+m+mi}{12}\\PY{o}{*}\\PY{n}{a}\\PY{o}{*}\\PY{n}{b}\\PY{o}{*}\\PY{n}{c} \\PY{o}{+} \\PY{l+m+mi}{3}\\PY{o}{*}\\PY{n}{x}\\PY{p}{[}\\PY{l+m+mi}{0}\\PY{p}{]}\\PY{o}{*}\\PY{o}{*}\\PY{l+m+mi}{3}\\PY{p}{)}\n    \n    \\PY{k}{def} \\PY{n+nf}{d\\PYZus{}f}\\PY{p}{(}\\PY{n}{x}\\PY{p}{)}\\PY{p}{:}\n        \\PY{k}{return} \\PY{p}{(}\\PY{n}{x}\\PY{o}{\\PYZhy{}}\\PY{n}{a}\\PY{p}{)}\\PY{o}{*}\\PY{p}{(}\\PY{n}{x}\\PY{o}{\\PYZhy{}}\\PY{n}{b}\\PY{p}{)}\\PY{o}{*}\\PY{p}{(}\\PY{n}{x}\\PY{o}{\\PYZhy{}}\\PY{n}{c}\\PY{p}{)}\n    \n    \\PY{k}{def} \\PY{n+nf}{d2\\PYZus{}f}\\PY{p}{(}\\PY{n}{x}\\PY{p}{)}\\PY{p}{:}\n        \\PY{k}{return} \\PY{n}{np}\\PY{o}{.}\\PY{k+kp}{array}\\PY{p}{(}\\PY{p}{[}\\PY{n}{a}\\PY{o}{*}\\PY{p}{(}\\PY{n}{b}\\PY{o}{+}\\PY{n}{c}\\PY{o}{\\PYZhy{}}\\PY{l+m+mi}{2}\\PY{o}{*}\\PY{n}{x}\\PY{p}{)} \\PY{o}{+} \\PY{n}{b}\\PY{o}{*}\\PY{p}{(}\\PY{n}{c}\\PY{o}{\\PYZhy{}}\\PY{l+m+mi}{2}\\PY{o}{*}\\PY{n}{x}\\PY{p}{)} \\PY{o}{+} \\PY{n}{x}\\PY{o}{*}\\PY{p}{(}\\PY{l+m+mi}{3}\\PY{o}{*}\\PY{n}{x}\\PY{o}{\\PYZhy{}}\\PY{l+m+mi}{2}\\PY{o}{*}\\PY{n}{c}\\PY{p}{)}\\PY{p}{]}\\PY{p}{)}\n\n    \\PY{k}{return} \\PY{n}{MinimizationProblem}\\PY{p}{(}\\PY{n}{f}\\PY{p}{,} \\PY{n}{d\\PYZus{}f}\\PY{p}{,} \\PY{n}{d2\\PYZus{}f}\\PY{p}{,} \\PY{n}{solution}\\PY{p}{)}\n\\end{Verbatim}\n"
     },
     "metadata": {},
     "execution_count": 11
    }
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "import numpy as np\n",
       "from dataclasses import dataclass, make_dataclass\n",
       "from typing import Callable, Tuple\n",
       "from decimal import Decimal\n",
       "\n",
       "from numpy.lib import gradient\n",
       "\n",
       "@dataclass\n",
       "class MinimizationProblem:\n",
       "    \"\"\" \n",
       "    Data class containing all necessary information of a minimization problem to support \n",
       "    steepest descent, newton, quasi-newton and conjugate minimization.\n",
       "    \"\"\"\n",
       "    f: Callable[[np.ndarray], np.ndarray]\n",
       "    gradient_f: Callable[[np.ndarray], np.ndarray]\n",
       "    hessian_f: Callable[[np.ndarray], np.ndarray]\n",
       "    solution: np.ndarray\n",
       "\n",
       "@dataclass\n",
       "class DirectionState:\n",
       "    \"\"\"\n",
       "    Data class containing the state of a direction calculation iteration, \n",
       "    holding interesting data for consumers and for the next iteration of the direction calculation.\n",
       "    \"\"\"\n",
       "    x: np.ndarray\n",
       "    direction: np.ndarray\n",
       "    gradient: np.ndarray\n",
       "\n",
       "@dataclass\n",
       "class BfgsQuasiNewtonState(DirectionState):\n",
       "    H: np.ndarray\n",
       "\n",
       "_epsilon = np.sqrt(np.finfo(float).eps)\n",
       "\n",
       "def find_minimizer(\n",
       "    problem: MinimizationProblem, \n",
       "    x0: np.ndarray, \n",
       "    direction_method: Callable[[np.ndarray, MinimizationProblem, DirectionState], DirectionState], \n",
       "    a0 = 1, \n",
       "    tolerance = 1e-5, \n",
       "    max_iter = 10_000):\n",
       "\n",
       "    x = x0\n",
       "\n",
       "    direction_state = None\n",
       "    gradients = []\n",
       "    for i in range(max_iter):\n",
       "        x, direction_state = _backtracking_line_search(problem, x, direction_state, direction_method, a0)\n",
       "        grad_norm = np.linalg.norm(direction_state.gradient)\n",
       "        gradients.append(grad_norm)\n",
       "\n",
       "        if grad_norm < tolerance:\n",
       "            break\n",
       "\n",
       "    return x, gradients\n",
       "\n",
       "# calculates conjugate direction with Fletcher-Reeves method\n",
       "def fr_conjugate_direction(x, problem: MinimizationProblem, prev_state: DirectionState):\n",
       "    grad = problem.gradient_f(x)\n",
       "    p = -grad\n",
       "    \n",
       "    if prev_state is not None:\n",
       "        prev_grad = prev_state.gradient\n",
       "        beta = (grad @ grad) / (prev_grad @ prev_grad)\n",
       "        p += beta * prev_state.direction\n",
       "\n",
       "    return DirectionState(x, p, grad)\n",
       "\n",
       "# calculates quasi-newton direction with BFGS method\n",
       "def bfgs_quasi_newton_direction(x, problem: MinimizationProblem, prev_state: BfgsQuasiNewtonState):\n",
       "    I = np.identity(len(x))\n",
       "    H = I\n",
       "    grad = problem.gradient_f(x)\n",
       "\n",
       "    if prev_state is not None:\n",
       "        s = x - prev_state.x\n",
       "        y = grad - prev_state.gradient\n",
       "        rho_denominator = y @ s\n",
       "\n",
       "        if rho_denominator != 0: # safety condition\n",
       "            rho = 1/rho_denominator\n",
       "            H = (I - rho * np.outer(s, y)) @ prev_state.H @ (I - rho * np.outer(y, s)) + rho * np.outer(s, s)\n",
       "\n",
       "    p = -H @ grad\n",
       "    return BfgsQuasiNewtonState(x, p, grad, H)\n",
       "\n",
       "# calculates steepest descent direction\n",
       "def steepest_descent_direction(x, problem: MinimizationProblem, prev_state: DirectionState):\n",
       "    grad = problem.gradient_f(x)\n",
       "    p = -grad\n",
       "\n",
       "    return DirectionState(x, p, grad)\n",
       "\n",
       "# calculates newton direction\n",
       "def newton_direction(x, problem: MinimizationProblem, prev_state: DirectionState):\n",
       "    hessian = problem.hessian_f(x)\n",
       "    hessian_inv = np.linalg.inv(hessian)\n",
       "    grad = problem.gradient_f(x)\n",
       "    p = -hessian_inv @ grad\n",
       "\n",
       "    return DirectionState(x, p, grad)\n",
       "\n",
       "# performs backtracking line search\n",
       "def _backtracking_line_search(\n",
       "    problem: MinimizationProblem, \n",
       "    x: np.ndarray, \n",
       "    prev_p_state: DirectionState,\n",
       "    direction_method: Callable[[np.ndarray, MinimizationProblem, DirectionState], DirectionState], \n",
       "    a0 = 1, c = 0.4, p = 0.8) -> Tuple[np.ndarray, DirectionState]:\n",
       "\n",
       "    a = a0\n",
       "\n",
       "    p_state = direction_method(x, problem, prev_p_state)\n",
       "\n",
       "    # first wolfe condition\n",
       "    while problem.f(x + a * p_state.direction) > (problem.f(x) + c * a * (p_state.gradient @ p_state.direction)):\n",
       "        a *= p \n",
       "\n",
       "        if a * np.linalg.norm(p_state.direction) < _epsilon:\n",
       "            # step must not become smaller than precision\n",
       "            break\n",
       "\n",
       "    return x + a * p_state.direction, p_state"
      ],
      "text/html": "<style>pre { line-height: 125%; }\ntd.linenos .normal { color: inherit; background-color: transparent; padding-left: 5px; padding-right: 5px; }\nspan.linenos { color: inherit; background-color: transparent; padding-left: 5px; padding-right: 5px; }\ntd.linenos .special { color: #000000; background-color: #ffffc0; padding-left: 5px; padding-right: 5px; }\nspan.linenos.special { color: #000000; background-color: #ffffc0; padding-left: 5px; padding-right: 5px; }\n.output_html .hll { background-color: #ffffcc }\n.output_html { background: #f8f8f8; }\n.output_html .c { color: #408080; font-style: italic } /* Comment */\n.output_html .err { border: 1px solid #FF0000 } /* Error */\n.output_html .k { color: #008000; font-weight: bold } /* Keyword */\n.output_html .o { color: #666666 } /* Operator */\n.output_html .ch { color: #408080; font-style: italic } /* Comment.Hashbang */\n.output_html .cm { color: #408080; font-style: italic } /* Comment.Multiline */\n.output_html .cp { color: #BC7A00 } /* Comment.Preproc */\n.output_html .cpf { color: #408080; font-style: italic } /* Comment.PreprocFile */\n.output_html .c1 { color: #408080; font-style: italic } /* Comment.Single */\n.output_html .cs { color: #408080; font-style: italic } /* Comment.Special */\n.output_html .gd { color: #A00000 } /* Generic.Deleted */\n.output_html .ge { font-style: italic } /* Generic.Emph */\n.output_html .gr { color: #FF0000 } /* Generic.Error */\n.output_html .gh { color: #000080; font-weight: bold } /* Generic.Heading */\n.output_html .gi { color: #00A000 } /* Generic.Inserted */\n.output_html .go { color: #888888 } /* Generic.Output */\n.output_html .gp { color: #000080; font-weight: bold } /* Generic.Prompt */\n.output_html .gs { font-weight: bold } /* Generic.Strong */\n.output_html .gu { color: #800080; font-weight: bold } /* Generic.Subheading */\n.output_html .gt { color: #0044DD } /* Generic.Traceback */\n.output_html .kc { color: #008000; font-weight: bold } /* Keyword.Constant */\n.output_html .kd { color: #008000; font-weight: bold } /* Keyword.Declaration */\n.output_html .kn { color: #008000; font-weight: bold } /* Keyword.Namespace */\n.output_html .kp { color: #008000 } /* Keyword.Pseudo */\n.output_html .kr { color: #008000; font-weight: bold } /* Keyword.Reserved */\n.output_html .kt { color: #B00040 } /* Keyword.Type */\n.output_html .m { color: #666666 } /* Literal.Number */\n.output_html .s { color: #BA2121 } /* Literal.String */\n.output_html .na { color: #7D9029 } /* Name.Attribute */\n.output_html .nb { color: #008000 } /* Name.Builtin */\n.output_html .nc { color: #0000FF; font-weight: bold } /* Name.Class */\n.output_html .no { color: #880000 } /* Name.Constant */\n.output_html .nd { color: #AA22FF } /* Name.Decorator */\n.output_html .ni { color: #999999; font-weight: bold } /* Name.Entity */\n.output_html .ne { color: #D2413A; font-weight: bold } /* Name.Exception */\n.output_html .nf { color: #0000FF } /* Name.Function */\n.output_html .nl { color: #A0A000 } /* Name.Label */\n.output_html .nn { color: #0000FF; font-weight: bold } /* Name.Namespace */\n.output_html .nt { color: #008000; font-weight: bold } /* Name.Tag */\n.output_html .nv { color: #19177C } /* Name.Variable */\n.output_html .ow { color: #AA22FF; font-weight: bold } /* Operator.Word */\n.output_html .w { color: #bbbbbb } /* Text.Whitespace */\n.output_html .mb { color: #666666 } /* Literal.Number.Bin */\n.output_html .mf { color: #666666 } /* Literal.Number.Float */\n.output_html .mh { color: #666666 } /* Literal.Number.Hex */\n.output_html .mi { color: #666666 } /* Literal.Number.Integer */\n.output_html .mo { color: #666666 } /* Literal.Number.Oct */\n.output_html .sa { color: #BA2121 } /* Literal.String.Affix */\n.output_html .sb { color: #BA2121 } /* Literal.String.Backtick */\n.output_html .sc { color: #BA2121 } /* Literal.String.Char */\n.output_html .dl { color: #BA2121 } /* Literal.String.Delimiter */\n.output_html .sd { color: #BA2121; font-style: italic } /* Literal.String.Doc */\n.output_html .s2 { color: #BA2121 } /* Literal.String.Double */\n.output_html .se { color: #BB6622; font-weight: bold } /* Literal.String.Escape */\n.output_html .sh { color: #BA2121 } /* Literal.String.Heredoc */\n.output_html .si { color: #BB6688; font-weight: bold } /* Literal.String.Interpol */\n.output_html .sx { color: #008000 } /* Literal.String.Other */\n.output_html .sr { color: #BB6688 } /* Literal.String.Regex */\n.output_html .s1 { color: #BA2121 } /* Literal.String.Single */\n.output_html .ss { color: #19177C } /* Literal.String.Symbol */\n.output_html .bp { color: #008000 } /* Name.Builtin.Pseudo */\n.output_html .fm { color: #0000FF } /* Name.Function.Magic */\n.output_html .vc { color: #19177C } /* Name.Variable.Class */\n.output_html .vg { color: #19177C } /* Name.Variable.Global */\n.output_html .vi { color: #19177C } /* Name.Variable.Instance */\n.output_html .vm { color: #19177C } /* Name.Variable.Magic */\n.output_html .il { color: #666666 } /* Literal.Number.Integer.Long */</style><div class=\"highlight\"><pre><span></span><span class=\"kn\">import</span> <span class=\"nn\">numpy</span> <span class=\"k\">as</span> <span class=\"nn\">np</span>\n<span class=\"kn\">from</span> <span class=\"nn\">dataclasses</span> <span class=\"kn\">import</span> <span class=\"n\">dataclass</span><span class=\"p\">,</span> <span class=\"n\">make_dataclass</span>\n<span class=\"kn\">from</span> <span class=\"nn\">typing</span> <span class=\"kn\">import</span> <span class=\"n\">Callable</span><span class=\"p\">,</span> <span class=\"n\">Tuple</span>\n<span class=\"kn\">from</span> <span class=\"nn\">decimal</span> <span class=\"kn\">import</span> <span class=\"n\">Decimal</span>\n\n<span class=\"kn\">from</span> <span class=\"nn\">numpy.lib</span> <span class=\"kn\">import</span> <span class=\"kp\">gradient</span>\n\n<span class=\"nd\">@dataclass</span>\n<span class=\"k\">class</span> <span class=\"nc\">MinimizationProblem</span><span class=\"p\">:</span>\n    <span class=\"sd\">&quot;&quot;&quot; </span>\n<span class=\"sd\">    Data class containing all necessary information of a minimization problem to support </span>\n<span class=\"sd\">    steepest descent, newton, quasi-newton and conjugate minimization.</span>\n<span class=\"sd\">    &quot;&quot;&quot;</span>\n    <span class=\"n\">f</span><span class=\"p\">:</span> <span class=\"n\">Callable</span><span class=\"p\">[[</span><span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">ndarray</span><span class=\"p\">],</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">ndarray</span><span class=\"p\">]</span>\n    <span class=\"n\">gradient_f</span><span class=\"p\">:</span> <span class=\"n\">Callable</span><span class=\"p\">[[</span><span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">ndarray</span><span class=\"p\">],</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">ndarray</span><span class=\"p\">]</span>\n    <span class=\"n\">hessian_f</span><span class=\"p\">:</span> <span class=\"n\">Callable</span><span class=\"p\">[[</span><span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">ndarray</span><span class=\"p\">],</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">ndarray</span><span class=\"p\">]</span>\n    <span class=\"n\">solution</span><span class=\"p\">:</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">ndarray</span>\n\n<span class=\"nd\">@dataclass</span>\n<span class=\"k\">class</span> <span class=\"nc\">DirectionState</span><span class=\"p\">:</span>\n    <span class=\"sd\">&quot;&quot;&quot;</span>\n<span class=\"sd\">    Data class containing the state of a direction calculation iteration, </span>\n<span class=\"sd\">    holding interesting data for consumers and for the next iteration of the direction calculation.</span>\n<span class=\"sd\">    &quot;&quot;&quot;</span>\n    <span class=\"n\">x</span><span class=\"p\">:</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">ndarray</span>\n    <span class=\"n\">direction</span><span class=\"p\">:</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">ndarray</span>\n    <span class=\"kp\">gradient</span><span class=\"p\">:</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">ndarray</span>\n\n<span class=\"nd\">@dataclass</span>\n<span class=\"k\">class</span> <span class=\"nc\">BfgsQuasiNewtonState</span><span class=\"p\">(</span><span class=\"n\">DirectionState</span><span class=\"p\">):</span>\n    <span class=\"n\">H</span><span class=\"p\">:</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">ndarray</span>\n\n<span class=\"n\">_epsilon</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"kp\">sqrt</span><span class=\"p\">(</span><span class=\"n\">np</span><span class=\"o\">.</span><span class=\"kp\">finfo</span><span class=\"p\">(</span><span class=\"nb\">float</span><span class=\"p\">)</span><span class=\"o\">.</span><span class=\"n\">eps</span><span class=\"p\">)</span>\n\n<span class=\"k\">def</span> <span class=\"nf\">find_minimizer</span><span class=\"p\">(</span>\n    <span class=\"n\">problem</span><span class=\"p\">:</span> <span class=\"n\">MinimizationProblem</span><span class=\"p\">,</span> \n    <span class=\"n\">x0</span><span class=\"p\">:</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">ndarray</span><span class=\"p\">,</span> \n    <span class=\"n\">direction_method</span><span class=\"p\">:</span> <span class=\"n\">Callable</span><span class=\"p\">[[</span><span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">ndarray</span><span class=\"p\">,</span> <span class=\"n\">MinimizationProblem</span><span class=\"p\">,</span> <span class=\"n\">DirectionState</span><span class=\"p\">],</span> <span class=\"n\">DirectionState</span><span class=\"p\">],</span> \n    <span class=\"n\">a0</span> <span class=\"o\">=</span> <span class=\"mi\">1</span><span class=\"p\">,</span> \n    <span class=\"n\">tolerance</span> <span class=\"o\">=</span> <span class=\"mf\">1e-5</span><span class=\"p\">,</span> \n    <span class=\"n\">max_iter</span> <span class=\"o\">=</span> <span class=\"mi\">10_000</span><span class=\"p\">):</span>\n\n    <span class=\"n\">x</span> <span class=\"o\">=</span> <span class=\"n\">x0</span>\n\n    <span class=\"n\">direction_state</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>\n    <span class=\"n\">gradients</span> <span class=\"o\">=</span> <span class=\"p\">[]</span>\n    <span class=\"k\">for</span> <span class=\"n\">i</span> <span class=\"ow\">in</span> <span class=\"nb\">range</span><span class=\"p\">(</span><span class=\"n\">max_iter</span><span class=\"p\">):</span>\n        <span class=\"n\">x</span><span class=\"p\">,</span> <span class=\"n\">direction_state</span> <span class=\"o\">=</span> <span class=\"n\">_backtracking_line_search</span><span class=\"p\">(</span><span class=\"n\">problem</span><span class=\"p\">,</span> <span class=\"n\">x</span><span class=\"p\">,</span> <span class=\"n\">direction_state</span><span class=\"p\">,</span> <span class=\"n\">direction_method</span><span class=\"p\">,</span> <span class=\"n\">a0</span><span class=\"p\">)</span>\n        <span class=\"n\">grad_norm</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">linalg</span><span class=\"o\">.</span><span class=\"n\">norm</span><span class=\"p\">(</span><span class=\"n\">direction_state</span><span class=\"o\">.</span><span class=\"kp\">gradient</span><span class=\"p\">)</span>\n        <span class=\"n\">gradients</span><span class=\"o\">.</span><span class=\"kp\">append</span><span class=\"p\">(</span><span class=\"n\">grad_norm</span><span class=\"p\">)</span>\n\n        <span class=\"k\">if</span> <span class=\"n\">grad_norm</span> <span class=\"o\">&lt;</span> <span class=\"n\">tolerance</span><span class=\"p\">:</span>\n            <span class=\"k\">break</span>\n\n    <span class=\"k\">return</span> <span class=\"n\">x</span><span class=\"p\">,</span> <span class=\"n\">gradients</span>\n\n<span class=\"c1\"># calculates conjugate direction with Fletcher-Reeves method</span>\n<span class=\"k\">def</span> <span class=\"nf\">fr_conjugate_direction</span><span class=\"p\">(</span><span class=\"n\">x</span><span class=\"p\">,</span> <span class=\"n\">problem</span><span class=\"p\">:</span> <span class=\"n\">MinimizationProblem</span><span class=\"p\">,</span> <span class=\"n\">prev_state</span><span class=\"p\">:</span> <span class=\"n\">DirectionState</span><span class=\"p\">):</span>\n    <span class=\"n\">grad</span> <span class=\"o\">=</span> <span class=\"n\">problem</span><span class=\"o\">.</span><span class=\"n\">gradient_f</span><span class=\"p\">(</span><span class=\"n\">x</span><span class=\"p\">)</span>\n    <span class=\"n\">p</span> <span class=\"o\">=</span> <span class=\"o\">-</span><span class=\"n\">grad</span>\n    \n    <span class=\"k\">if</span> <span class=\"n\">prev_state</span> <span class=\"ow\">is</span> <span class=\"ow\">not</span> <span class=\"kc\">None</span><span class=\"p\">:</span>\n        <span class=\"n\">prev_grad</span> <span class=\"o\">=</span> <span class=\"n\">prev_state</span><span class=\"o\">.</span><span class=\"kp\">gradient</span>\n        <span class=\"kp\">beta</span> <span class=\"o\">=</span> <span class=\"p\">(</span><span class=\"n\">grad</span> <span class=\"o\">@</span> <span class=\"n\">grad</span><span class=\"p\">)</span> <span class=\"o\">/</span> <span class=\"p\">(</span><span class=\"n\">prev_grad</span> <span class=\"o\">@</span> <span class=\"n\">prev_grad</span><span class=\"p\">)</span>\n        <span class=\"n\">p</span> <span class=\"o\">+=</span> <span class=\"kp\">beta</span> <span class=\"o\">*</span> <span class=\"n\">prev_state</span><span class=\"o\">.</span><span class=\"n\">direction</span>\n\n    <span class=\"k\">return</span> <span class=\"n\">DirectionState</span><span class=\"p\">(</span><span class=\"n\">x</span><span class=\"p\">,</span> <span class=\"n\">p</span><span class=\"p\">,</span> <span class=\"n\">grad</span><span class=\"p\">)</span>\n\n<span class=\"c1\"># calculates quasi-newton direction with BFGS method</span>\n<span class=\"k\">def</span> <span class=\"nf\">bfgs_quasi_newton_direction</span><span class=\"p\">(</span><span class=\"n\">x</span><span class=\"p\">,</span> <span class=\"n\">problem</span><span class=\"p\">:</span> <span class=\"n\">MinimizationProblem</span><span class=\"p\">,</span> <span class=\"n\">prev_state</span><span class=\"p\">:</span> <span class=\"n\">BfgsQuasiNewtonState</span><span class=\"p\">):</span>\n    <span class=\"n\">I</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"kp\">identity</span><span class=\"p\">(</span><span class=\"nb\">len</span><span class=\"p\">(</span><span class=\"n\">x</span><span class=\"p\">))</span>\n    <span class=\"n\">H</span> <span class=\"o\">=</span> <span class=\"n\">I</span>\n    <span class=\"n\">grad</span> <span class=\"o\">=</span> <span class=\"n\">problem</span><span class=\"o\">.</span><span class=\"n\">gradient_f</span><span class=\"p\">(</span><span class=\"n\">x</span><span class=\"p\">)</span>\n\n    <span class=\"k\">if</span> <span class=\"n\">prev_state</span> <span class=\"ow\">is</span> <span class=\"ow\">not</span> <span class=\"kc\">None</span><span class=\"p\">:</span>\n        <span class=\"n\">s</span> <span class=\"o\">=</span> <span class=\"n\">x</span> <span class=\"o\">-</span> <span class=\"n\">prev_state</span><span class=\"o\">.</span><span class=\"n\">x</span>\n        <span class=\"n\">y</span> <span class=\"o\">=</span> <span class=\"n\">grad</span> <span class=\"o\">-</span> <span class=\"n\">prev_state</span><span class=\"o\">.</span><span class=\"kp\">gradient</span>\n        <span class=\"n\">rho_denominator</span> <span class=\"o\">=</span> <span class=\"n\">y</span> <span class=\"o\">@</span> <span class=\"n\">s</span>\n\n        <span class=\"k\">if</span> <span class=\"n\">rho_denominator</span> <span class=\"o\">!=</span> <span class=\"mi\">0</span><span class=\"p\">:</span> <span class=\"c1\"># safety condition</span>\n            <span class=\"n\">rho</span> <span class=\"o\">=</span> <span class=\"mi\">1</span><span class=\"o\">/</span><span class=\"n\">rho_denominator</span>\n            <span class=\"n\">H</span> <span class=\"o\">=</span> <span class=\"p\">(</span><span class=\"n\">I</span> <span class=\"o\">-</span> <span class=\"n\">rho</span> <span class=\"o\">*</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"kp\">outer</span><span class=\"p\">(</span><span class=\"n\">s</span><span class=\"p\">,</span> <span class=\"n\">y</span><span class=\"p\">))</span> <span class=\"o\">@</span> <span class=\"n\">prev_state</span><span class=\"o\">.</span><span class=\"n\">H</span> <span class=\"o\">@</span> <span class=\"p\">(</span><span class=\"n\">I</span> <span class=\"o\">-</span> <span class=\"n\">rho</span> <span class=\"o\">*</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"kp\">outer</span><span class=\"p\">(</span><span class=\"n\">y</span><span class=\"p\">,</span> <span class=\"n\">s</span><span class=\"p\">))</span> <span class=\"o\">+</span> <span class=\"n\">rho</span> <span class=\"o\">*</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"kp\">outer</span><span class=\"p\">(</span><span class=\"n\">s</span><span class=\"p\">,</span> <span class=\"n\">s</span><span class=\"p\">)</span>\n\n    <span class=\"n\">p</span> <span class=\"o\">=</span> <span class=\"o\">-</span><span class=\"n\">H</span> <span class=\"o\">@</span> <span class=\"n\">grad</span>\n    <span class=\"k\">return</span> <span class=\"n\">BfgsQuasiNewtonState</span><span class=\"p\">(</span><span class=\"n\">x</span><span class=\"p\">,</span> <span class=\"n\">p</span><span class=\"p\">,</span> <span class=\"n\">grad</span><span class=\"p\">,</span> <span class=\"n\">H</span><span class=\"p\">)</span>\n\n<span class=\"c1\"># calculates steepest descent direction</span>\n<span class=\"k\">def</span> <span class=\"nf\">steepest_descent_direction</span><span class=\"p\">(</span><span class=\"n\">x</span><span class=\"p\">,</span> <span class=\"n\">problem</span><span class=\"p\">:</span> <span class=\"n\">MinimizationProblem</span><span class=\"p\">,</span> <span class=\"n\">prev_state</span><span class=\"p\">:</span> <span class=\"n\">DirectionState</span><span class=\"p\">):</span>\n    <span class=\"n\">grad</span> <span class=\"o\">=</span> <span class=\"n\">problem</span><span class=\"o\">.</span><span class=\"n\">gradient_f</span><span class=\"p\">(</span><span class=\"n\">x</span><span class=\"p\">)</span>\n    <span class=\"n\">p</span> <span class=\"o\">=</span> <span class=\"o\">-</span><span class=\"n\">grad</span>\n\n    <span class=\"k\">return</span> <span class=\"n\">DirectionState</span><span class=\"p\">(</span><span class=\"n\">x</span><span class=\"p\">,</span> <span class=\"n\">p</span><span class=\"p\">,</span> <span class=\"n\">grad</span><span class=\"p\">)</span>\n\n<span class=\"c1\"># calculates newton direction</span>\n<span class=\"k\">def</span> <span class=\"nf\">newton_direction</span><span class=\"p\">(</span><span class=\"n\">x</span><span class=\"p\">,</span> <span class=\"n\">problem</span><span class=\"p\">:</span> <span class=\"n\">MinimizationProblem</span><span class=\"p\">,</span> <span class=\"n\">prev_state</span><span class=\"p\">:</span> <span class=\"n\">DirectionState</span><span class=\"p\">):</span>\n    <span class=\"n\">hessian</span> <span class=\"o\">=</span> <span class=\"n\">problem</span><span class=\"o\">.</span><span class=\"n\">hessian_f</span><span class=\"p\">(</span><span class=\"n\">x</span><span class=\"p\">)</span>\n    <span class=\"n\">hessian_inv</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">linalg</span><span class=\"o\">.</span><span class=\"kp\">inv</span><span class=\"p\">(</span><span class=\"n\">hessian</span><span class=\"p\">)</span>\n    <span class=\"n\">grad</span> <span class=\"o\">=</span> <span class=\"n\">problem</span><span class=\"o\">.</span><span class=\"n\">gradient_f</span><span class=\"p\">(</span><span class=\"n\">x</span><span class=\"p\">)</span>\n    <span class=\"n\">p</span> <span class=\"o\">=</span> <span class=\"o\">-</span><span class=\"n\">hessian_inv</span> <span class=\"o\">@</span> <span class=\"n\">grad</span>\n\n    <span class=\"k\">return</span> <span class=\"n\">DirectionState</span><span class=\"p\">(</span><span class=\"n\">x</span><span class=\"p\">,</span> <span class=\"n\">p</span><span class=\"p\">,</span> <span class=\"n\">grad</span><span class=\"p\">)</span>\n\n<span class=\"c1\"># performs backtracking line search</span>\n<span class=\"k\">def</span> <span class=\"nf\">_backtracking_line_search</span><span class=\"p\">(</span>\n    <span class=\"n\">problem</span><span class=\"p\">:</span> <span class=\"n\">MinimizationProblem</span><span class=\"p\">,</span> \n    <span class=\"n\">x</span><span class=\"p\">:</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">ndarray</span><span class=\"p\">,</span> \n    <span class=\"n\">prev_p_state</span><span class=\"p\">:</span> <span class=\"n\">DirectionState</span><span class=\"p\">,</span>\n    <span class=\"n\">direction_method</span><span class=\"p\">:</span> <span class=\"n\">Callable</span><span class=\"p\">[[</span><span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">ndarray</span><span class=\"p\">,</span> <span class=\"n\">MinimizationProblem</span><span class=\"p\">,</span> <span class=\"n\">DirectionState</span><span class=\"p\">],</span> <span class=\"n\">DirectionState</span><span class=\"p\">],</span> \n    <span class=\"n\">a0</span> <span class=\"o\">=</span> <span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"n\">c</span> <span class=\"o\">=</span> <span class=\"mf\">0.4</span><span class=\"p\">,</span> <span class=\"n\">p</span> <span class=\"o\">=</span> <span class=\"mf\">0.8</span><span class=\"p\">)</span> <span class=\"o\">-&gt;</span> <span class=\"n\">Tuple</span><span class=\"p\">[</span><span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">ndarray</span><span class=\"p\">,</span> <span class=\"n\">DirectionState</span><span class=\"p\">]:</span>\n\n    <span class=\"n\">a</span> <span class=\"o\">=</span> <span class=\"n\">a0</span>\n\n    <span class=\"n\">p_state</span> <span class=\"o\">=</span> <span class=\"n\">direction_method</span><span class=\"p\">(</span><span class=\"n\">x</span><span class=\"p\">,</span> <span class=\"n\">problem</span><span class=\"p\">,</span> <span class=\"n\">prev_p_state</span><span class=\"p\">)</span>\n\n    <span class=\"c1\"># first wolfe condition</span>\n    <span class=\"k\">while</span> <span class=\"n\">problem</span><span class=\"o\">.</span><span class=\"n\">f</span><span class=\"p\">(</span><span class=\"n\">x</span> <span class=\"o\">+</span> <span class=\"n\">a</span> <span class=\"o\">*</span> <span class=\"n\">p_state</span><span class=\"o\">.</span><span class=\"n\">direction</span><span class=\"p\">)</span> <span class=\"o\">&gt;</span> <span class=\"p\">(</span><span class=\"n\">problem</span><span class=\"o\">.</span><span class=\"n\">f</span><span class=\"p\">(</span><span class=\"n\">x</span><span class=\"p\">)</span> <span class=\"o\">+</span> <span class=\"n\">c</span> <span class=\"o\">*</span> <span class=\"n\">a</span> <span class=\"o\">*</span> <span class=\"p\">(</span><span class=\"n\">p_state</span><span class=\"o\">.</span><span class=\"kp\">gradient</span> <span class=\"o\">@</span> <span class=\"n\">p_state</span><span class=\"o\">.</span><span class=\"n\">direction</span><span class=\"p\">)):</span>\n        <span class=\"n\">a</span> <span class=\"o\">*=</span> <span class=\"n\">p</span> \n\n        <span class=\"k\">if</span> <span class=\"n\">a</span> <span class=\"o\">*</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">linalg</span><span class=\"o\">.</span><span class=\"n\">norm</span><span class=\"p\">(</span><span class=\"n\">p_state</span><span class=\"o\">.</span><span class=\"n\">direction</span><span class=\"p\">)</span> <span class=\"o\">&lt;</span> <span class=\"n\">_epsilon</span><span class=\"p\">:</span>\n            <span class=\"c1\"># step must not become smaller than precision</span>\n            <span class=\"k\">break</span>\n\n    <span class=\"k\">return</span> <span class=\"n\">x</span> <span class=\"o\">+</span> <span class=\"n\">a</span> <span class=\"o\">*</span> <span class=\"n\">p_state</span><span class=\"o\">.</span><span class=\"n\">direction</span><span class=\"p\">,</span> <span class=\"n\">p_state</span>\n</pre></div>\n",
      "text/latex": "\\begin{Verbatim}[commandchars=\\\\\\{\\}]\n\\PY{k+kn}{import} \\PY{n+nn}{numpy} \\PY{k}{as} \\PY{n+nn}{np}\n\\PY{k+kn}{from} \\PY{n+nn}{dataclasses} \\PY{k+kn}{import} \\PY{n}{dataclass}\\PY{p}{,} \\PY{n}{make\\PYZus{}dataclass}\n\\PY{k+kn}{from} \\PY{n+nn}{typing} \\PY{k+kn}{import} \\PY{n}{Callable}\\PY{p}{,} \\PY{n}{Tuple}\n\\PY{k+kn}{from} \\PY{n+nn}{decimal} \\PY{k+kn}{import} \\PY{n}{Decimal}\n\n\\PY{k+kn}{from} \\PY{n+nn}{numpy}\\PY{n+nn}{.}\\PY{n+nn}{lib} \\PY{k+kn}{import} \\PY{k+kp}{gradient}\n\n\\PY{n+nd}{@dataclass}\n\\PY{k}{class} \\PY{n+nc}{MinimizationProblem}\\PY{p}{:}\n    \\PY{l+s+sd}{\\PYZdq{}\\PYZdq{}\\PYZdq{} }\n\\PY{l+s+sd}{    Data class containing all necessary information of a minimization problem to support }\n\\PY{l+s+sd}{    steepest descent, newton, quasi\\PYZhy{}newton and conjugate minimization.}\n\\PY{l+s+sd}{    \\PYZdq{}\\PYZdq{}\\PYZdq{}}\n    \\PY{n}{f}\\PY{p}{:} \\PY{n}{Callable}\\PY{p}{[}\\PY{p}{[}\\PY{n}{np}\\PY{o}{.}\\PY{n}{ndarray}\\PY{p}{]}\\PY{p}{,} \\PY{n}{np}\\PY{o}{.}\\PY{n}{ndarray}\\PY{p}{]}\n    \\PY{n}{gradient\\PYZus{}f}\\PY{p}{:} \\PY{n}{Callable}\\PY{p}{[}\\PY{p}{[}\\PY{n}{np}\\PY{o}{.}\\PY{n}{ndarray}\\PY{p}{]}\\PY{p}{,} \\PY{n}{np}\\PY{o}{.}\\PY{n}{ndarray}\\PY{p}{]}\n    \\PY{n}{hessian\\PYZus{}f}\\PY{p}{:} \\PY{n}{Callable}\\PY{p}{[}\\PY{p}{[}\\PY{n}{np}\\PY{o}{.}\\PY{n}{ndarray}\\PY{p}{]}\\PY{p}{,} \\PY{n}{np}\\PY{o}{.}\\PY{n}{ndarray}\\PY{p}{]}\n    \\PY{n}{solution}\\PY{p}{:} \\PY{n}{np}\\PY{o}{.}\\PY{n}{ndarray}\n\n\\PY{n+nd}{@dataclass}\n\\PY{k}{class} \\PY{n+nc}{DirectionState}\\PY{p}{:}\n    \\PY{l+s+sd}{\\PYZdq{}\\PYZdq{}\\PYZdq{}}\n\\PY{l+s+sd}{    Data class containing the state of a direction calculation iteration, }\n\\PY{l+s+sd}{    holding interesting data for consumers and for the next iteration of the direction calculation.}\n\\PY{l+s+sd}{    \\PYZdq{}\\PYZdq{}\\PYZdq{}}\n    \\PY{n}{x}\\PY{p}{:} \\PY{n}{np}\\PY{o}{.}\\PY{n}{ndarray}\n    \\PY{n}{direction}\\PY{p}{:} \\PY{n}{np}\\PY{o}{.}\\PY{n}{ndarray}\n    \\PY{k+kp}{gradient}\\PY{p}{:} \\PY{n}{np}\\PY{o}{.}\\PY{n}{ndarray}\n\n\\PY{n+nd}{@dataclass}\n\\PY{k}{class} \\PY{n+nc}{BfgsQuasiNewtonState}\\PY{p}{(}\\PY{n}{DirectionState}\\PY{p}{)}\\PY{p}{:}\n    \\PY{n}{H}\\PY{p}{:} \\PY{n}{np}\\PY{o}{.}\\PY{n}{ndarray}\n\n\\PY{n}{\\PYZus{}epsilon} \\PY{o}{=} \\PY{n}{np}\\PY{o}{.}\\PY{k+kp}{sqrt}\\PY{p}{(}\\PY{n}{np}\\PY{o}{.}\\PY{k+kp}{finfo}\\PY{p}{(}\\PY{n+nb}{float}\\PY{p}{)}\\PY{o}{.}\\PY{n}{eps}\\PY{p}{)}\n\n\\PY{k}{def} \\PY{n+nf}{find\\PYZus{}minimizer}\\PY{p}{(}\n    \\PY{n}{problem}\\PY{p}{:} \\PY{n}{MinimizationProblem}\\PY{p}{,} \n    \\PY{n}{x0}\\PY{p}{:} \\PY{n}{np}\\PY{o}{.}\\PY{n}{ndarray}\\PY{p}{,} \n    \\PY{n}{direction\\PYZus{}method}\\PY{p}{:} \\PY{n}{Callable}\\PY{p}{[}\\PY{p}{[}\\PY{n}{np}\\PY{o}{.}\\PY{n}{ndarray}\\PY{p}{,} \\PY{n}{MinimizationProblem}\\PY{p}{,} \\PY{n}{DirectionState}\\PY{p}{]}\\PY{p}{,} \\PY{n}{DirectionState}\\PY{p}{]}\\PY{p}{,} \n    \\PY{n}{a0} \\PY{o}{=} \\PY{l+m+mi}{1}\\PY{p}{,} \n    \\PY{n}{tolerance} \\PY{o}{=} \\PY{l+m+mf}{1e\\PYZhy{}5}\\PY{p}{,} \n    \\PY{n}{max\\PYZus{}iter} \\PY{o}{=} \\PY{l+m+mi}{10\\PYZus{}000}\\PY{p}{)}\\PY{p}{:}\n\n    \\PY{n}{x} \\PY{o}{=} \\PY{n}{x0}\n\n    \\PY{n}{direction\\PYZus{}state} \\PY{o}{=} \\PY{k+kc}{None}\n    \\PY{n}{gradients} \\PY{o}{=} \\PY{p}{[}\\PY{p}{]}\n    \\PY{k}{for} \\PY{n}{i} \\PY{o+ow}{in} \\PY{n+nb}{range}\\PY{p}{(}\\PY{n}{max\\PYZus{}iter}\\PY{p}{)}\\PY{p}{:}\n        \\PY{n}{x}\\PY{p}{,} \\PY{n}{direction\\PYZus{}state} \\PY{o}{=} \\PY{n}{\\PYZus{}backtracking\\PYZus{}line\\PYZus{}search}\\PY{p}{(}\\PY{n}{problem}\\PY{p}{,} \\PY{n}{x}\\PY{p}{,} \\PY{n}{direction\\PYZus{}state}\\PY{p}{,} \\PY{n}{direction\\PYZus{}method}\\PY{p}{,} \\PY{n}{a0}\\PY{p}{)}\n        \\PY{n}{grad\\PYZus{}norm} \\PY{o}{=} \\PY{n}{np}\\PY{o}{.}\\PY{n}{linalg}\\PY{o}{.}\\PY{n}{norm}\\PY{p}{(}\\PY{n}{direction\\PYZus{}state}\\PY{o}{.}\\PY{k+kp}{gradient}\\PY{p}{)}\n        \\PY{n}{gradients}\\PY{o}{.}\\PY{k+kp}{append}\\PY{p}{(}\\PY{n}{grad\\PYZus{}norm}\\PY{p}{)}\n\n        \\PY{k}{if} \\PY{n}{grad\\PYZus{}norm} \\PY{o}{\\PYZlt{}} \\PY{n}{tolerance}\\PY{p}{:}\n            \\PY{k}{break}\n\n    \\PY{k}{return} \\PY{n}{x}\\PY{p}{,} \\PY{n}{gradients}\n\n\\PY{c+c1}{\\PYZsh{} calculates conjugate direction with Fletcher\\PYZhy{}Reeves method}\n\\PY{k}{def} \\PY{n+nf}{fr\\PYZus{}conjugate\\PYZus{}direction}\\PY{p}{(}\\PY{n}{x}\\PY{p}{,} \\PY{n}{problem}\\PY{p}{:} \\PY{n}{MinimizationProblem}\\PY{p}{,} \\PY{n}{prev\\PYZus{}state}\\PY{p}{:} \\PY{n}{DirectionState}\\PY{p}{)}\\PY{p}{:}\n    \\PY{n}{grad} \\PY{o}{=} \\PY{n}{problem}\\PY{o}{.}\\PY{n}{gradient\\PYZus{}f}\\PY{p}{(}\\PY{n}{x}\\PY{p}{)}\n    \\PY{n}{p} \\PY{o}{=} \\PY{o}{\\PYZhy{}}\\PY{n}{grad}\n    \n    \\PY{k}{if} \\PY{n}{prev\\PYZus{}state} \\PY{o+ow}{is} \\PY{o+ow}{not} \\PY{k+kc}{None}\\PY{p}{:}\n        \\PY{n}{prev\\PYZus{}grad} \\PY{o}{=} \\PY{n}{prev\\PYZus{}state}\\PY{o}{.}\\PY{k+kp}{gradient}\n        \\PY{k+kp}{beta} \\PY{o}{=} \\PY{p}{(}\\PY{n}{grad} \\PY{o}{@} \\PY{n}{grad}\\PY{p}{)} \\PY{o}{/} \\PY{p}{(}\\PY{n}{prev\\PYZus{}grad} \\PY{o}{@} \\PY{n}{prev\\PYZus{}grad}\\PY{p}{)}\n        \\PY{n}{p} \\PY{o}{+}\\PY{o}{=} \\PY{k+kp}{beta} \\PY{o}{*} \\PY{n}{prev\\PYZus{}state}\\PY{o}{.}\\PY{n}{direction}\n\n    \\PY{k}{return} \\PY{n}{DirectionState}\\PY{p}{(}\\PY{n}{x}\\PY{p}{,} \\PY{n}{p}\\PY{p}{,} \\PY{n}{grad}\\PY{p}{)}\n\n\\PY{c+c1}{\\PYZsh{} calculates quasi\\PYZhy{}newton direction with BFGS method}\n\\PY{k}{def} \\PY{n+nf}{bfgs\\PYZus{}quasi\\PYZus{}newton\\PYZus{}direction}\\PY{p}{(}\\PY{n}{x}\\PY{p}{,} \\PY{n}{problem}\\PY{p}{:} \\PY{n}{MinimizationProblem}\\PY{p}{,} \\PY{n}{prev\\PYZus{}state}\\PY{p}{:} \\PY{n}{BfgsQuasiNewtonState}\\PY{p}{)}\\PY{p}{:}\n    \\PY{n}{I} \\PY{o}{=} \\PY{n}{np}\\PY{o}{.}\\PY{k+kp}{identity}\\PY{p}{(}\\PY{n+nb}{len}\\PY{p}{(}\\PY{n}{x}\\PY{p}{)}\\PY{p}{)}\n    \\PY{n}{H} \\PY{o}{=} \\PY{n}{I}\n    \\PY{n}{grad} \\PY{o}{=} \\PY{n}{problem}\\PY{o}{.}\\PY{n}{gradient\\PYZus{}f}\\PY{p}{(}\\PY{n}{x}\\PY{p}{)}\n\n    \\PY{k}{if} \\PY{n}{prev\\PYZus{}state} \\PY{o+ow}{is} \\PY{o+ow}{not} \\PY{k+kc}{None}\\PY{p}{:}\n        \\PY{n}{s} \\PY{o}{=} \\PY{n}{x} \\PY{o}{\\PYZhy{}} \\PY{n}{prev\\PYZus{}state}\\PY{o}{.}\\PY{n}{x}\n        \\PY{n}{y} \\PY{o}{=} \\PY{n}{grad} \\PY{o}{\\PYZhy{}} \\PY{n}{prev\\PYZus{}state}\\PY{o}{.}\\PY{k+kp}{gradient}\n        \\PY{n}{rho\\PYZus{}denominator} \\PY{o}{=} \\PY{n}{y} \\PY{o}{@} \\PY{n}{s}\n\n        \\PY{k}{if} \\PY{n}{rho\\PYZus{}denominator} \\PY{o}{!=} \\PY{l+m+mi}{0}\\PY{p}{:} \\PY{c+c1}{\\PYZsh{} safety condition}\n            \\PY{n}{rho} \\PY{o}{=} \\PY{l+m+mi}{1}\\PY{o}{/}\\PY{n}{rho\\PYZus{}denominator}\n            \\PY{n}{H} \\PY{o}{=} \\PY{p}{(}\\PY{n}{I} \\PY{o}{\\PYZhy{}} \\PY{n}{rho} \\PY{o}{*} \\PY{n}{np}\\PY{o}{.}\\PY{k+kp}{outer}\\PY{p}{(}\\PY{n}{s}\\PY{p}{,} \\PY{n}{y}\\PY{p}{)}\\PY{p}{)} \\PY{o}{@} \\PY{n}{prev\\PYZus{}state}\\PY{o}{.}\\PY{n}{H} \\PY{o}{@} \\PY{p}{(}\\PY{n}{I} \\PY{o}{\\PYZhy{}} \\PY{n}{rho} \\PY{o}{*} \\PY{n}{np}\\PY{o}{.}\\PY{k+kp}{outer}\\PY{p}{(}\\PY{n}{y}\\PY{p}{,} \\PY{n}{s}\\PY{p}{)}\\PY{p}{)} \\PY{o}{+} \\PY{n}{rho} \\PY{o}{*} \\PY{n}{np}\\PY{o}{.}\\PY{k+kp}{outer}\\PY{p}{(}\\PY{n}{s}\\PY{p}{,} \\PY{n}{s}\\PY{p}{)}\n\n    \\PY{n}{p} \\PY{o}{=} \\PY{o}{\\PYZhy{}}\\PY{n}{H} \\PY{o}{@} \\PY{n}{grad}\n    \\PY{k}{return} \\PY{n}{BfgsQuasiNewtonState}\\PY{p}{(}\\PY{n}{x}\\PY{p}{,} \\PY{n}{p}\\PY{p}{,} \\PY{n}{grad}\\PY{p}{,} \\PY{n}{H}\\PY{p}{)}\n\n\\PY{c+c1}{\\PYZsh{} calculates steepest descent direction}\n\\PY{k}{def} \\PY{n+nf}{steepest\\PYZus{}descent\\PYZus{}direction}\\PY{p}{(}\\PY{n}{x}\\PY{p}{,} \\PY{n}{problem}\\PY{p}{:} \\PY{n}{MinimizationProblem}\\PY{p}{,} \\PY{n}{prev\\PYZus{}state}\\PY{p}{:} \\PY{n}{DirectionState}\\PY{p}{)}\\PY{p}{:}\n    \\PY{n}{grad} \\PY{o}{=} \\PY{n}{problem}\\PY{o}{.}\\PY{n}{gradient\\PYZus{}f}\\PY{p}{(}\\PY{n}{x}\\PY{p}{)}\n    \\PY{n}{p} \\PY{o}{=} \\PY{o}{\\PYZhy{}}\\PY{n}{grad}\n\n    \\PY{k}{return} \\PY{n}{DirectionState}\\PY{p}{(}\\PY{n}{x}\\PY{p}{,} \\PY{n}{p}\\PY{p}{,} \\PY{n}{grad}\\PY{p}{)}\n\n\\PY{c+c1}{\\PYZsh{} calculates newton direction}\n\\PY{k}{def} \\PY{n+nf}{newton\\PYZus{}direction}\\PY{p}{(}\\PY{n}{x}\\PY{p}{,} \\PY{n}{problem}\\PY{p}{:} \\PY{n}{MinimizationProblem}\\PY{p}{,} \\PY{n}{prev\\PYZus{}state}\\PY{p}{:} \\PY{n}{DirectionState}\\PY{p}{)}\\PY{p}{:}\n    \\PY{n}{hessian} \\PY{o}{=} \\PY{n}{problem}\\PY{o}{.}\\PY{n}{hessian\\PYZus{}f}\\PY{p}{(}\\PY{n}{x}\\PY{p}{)}\n    \\PY{n}{hessian\\PYZus{}inv} \\PY{o}{=} \\PY{n}{np}\\PY{o}{.}\\PY{n}{linalg}\\PY{o}{.}\\PY{k+kp}{inv}\\PY{p}{(}\\PY{n}{hessian}\\PY{p}{)}\n    \\PY{n}{grad} \\PY{o}{=} \\PY{n}{problem}\\PY{o}{.}\\PY{n}{gradient\\PYZus{}f}\\PY{p}{(}\\PY{n}{x}\\PY{p}{)}\n    \\PY{n}{p} \\PY{o}{=} \\PY{o}{\\PYZhy{}}\\PY{n}{hessian\\PYZus{}inv} \\PY{o}{@} \\PY{n}{grad}\n\n    \\PY{k}{return} \\PY{n}{DirectionState}\\PY{p}{(}\\PY{n}{x}\\PY{p}{,} \\PY{n}{p}\\PY{p}{,} \\PY{n}{grad}\\PY{p}{)}\n\n\\PY{c+c1}{\\PYZsh{} performs backtracking line search}\n\\PY{k}{def} \\PY{n+nf}{\\PYZus{}backtracking\\PYZus{}line\\PYZus{}search}\\PY{p}{(}\n    \\PY{n}{problem}\\PY{p}{:} \\PY{n}{MinimizationProblem}\\PY{p}{,} \n    \\PY{n}{x}\\PY{p}{:} \\PY{n}{np}\\PY{o}{.}\\PY{n}{ndarray}\\PY{p}{,} \n    \\PY{n}{prev\\PYZus{}p\\PYZus{}state}\\PY{p}{:} \\PY{n}{DirectionState}\\PY{p}{,}\n    \\PY{n}{direction\\PYZus{}method}\\PY{p}{:} \\PY{n}{Callable}\\PY{p}{[}\\PY{p}{[}\\PY{n}{np}\\PY{o}{.}\\PY{n}{ndarray}\\PY{p}{,} \\PY{n}{MinimizationProblem}\\PY{p}{,} \\PY{n}{DirectionState}\\PY{p}{]}\\PY{p}{,} \\PY{n}{DirectionState}\\PY{p}{]}\\PY{p}{,} \n    \\PY{n}{a0} \\PY{o}{=} \\PY{l+m+mi}{1}\\PY{p}{,} \\PY{n}{c} \\PY{o}{=} \\PY{l+m+mf}{0.4}\\PY{p}{,} \\PY{n}{p} \\PY{o}{=} \\PY{l+m+mf}{0.8}\\PY{p}{)} \\PY{o}{\\PYZhy{}}\\PY{o}{\\PYZgt{}} \\PY{n}{Tuple}\\PY{p}{[}\\PY{n}{np}\\PY{o}{.}\\PY{n}{ndarray}\\PY{p}{,} \\PY{n}{DirectionState}\\PY{p}{]}\\PY{p}{:}\n\n    \\PY{n}{a} \\PY{o}{=} \\PY{n}{a0}\n\n    \\PY{n}{p\\PYZus{}state} \\PY{o}{=} \\PY{n}{direction\\PYZus{}method}\\PY{p}{(}\\PY{n}{x}\\PY{p}{,} \\PY{n}{problem}\\PY{p}{,} \\PY{n}{prev\\PYZus{}p\\PYZus{}state}\\PY{p}{)}\n\n    \\PY{c+c1}{\\PYZsh{} first wolfe condition}\n    \\PY{k}{while} \\PY{n}{problem}\\PY{o}{.}\\PY{n}{f}\\PY{p}{(}\\PY{n}{x} \\PY{o}{+} \\PY{n}{a} \\PY{o}{*} \\PY{n}{p\\PYZus{}state}\\PY{o}{.}\\PY{n}{direction}\\PY{p}{)} \\PY{o}{\\PYZgt{}} \\PY{p}{(}\\PY{n}{problem}\\PY{o}{.}\\PY{n}{f}\\PY{p}{(}\\PY{n}{x}\\PY{p}{)} \\PY{o}{+} \\PY{n}{c} \\PY{o}{*} \\PY{n}{a} \\PY{o}{*} \\PY{p}{(}\\PY{n}{p\\PYZus{}state}\\PY{o}{.}\\PY{k+kp}{gradient} \\PY{o}{@} \\PY{n}{p\\PYZus{}state}\\PY{o}{.}\\PY{n}{direction}\\PY{p}{)}\\PY{p}{)}\\PY{p}{:}\n        \\PY{n}{a} \\PY{o}{*}\\PY{o}{=} \\PY{n}{p} \n\n        \\PY{k}{if} \\PY{n}{a} \\PY{o}{*} \\PY{n}{np}\\PY{o}{.}\\PY{n}{linalg}\\PY{o}{.}\\PY{n}{norm}\\PY{p}{(}\\PY{n}{p\\PYZus{}state}\\PY{o}{.}\\PY{n}{direction}\\PY{p}{)} \\PY{o}{\\PYZlt{}} \\PY{n}{\\PYZus{}epsilon}\\PY{p}{:}\n            \\PY{c+c1}{\\PYZsh{} step must not become smaller than precision}\n            \\PY{k}{break}\n\n    \\PY{k}{return} \\PY{n}{x} \\PY{o}{+} \\PY{n}{a} \\PY{o}{*} \\PY{n}{p\\PYZus{}state}\\PY{o}{.}\\PY{n}{direction}\\PY{p}{,} \\PY{n}{p\\PYZus{}state}\n\\end{Verbatim}\n"
     },
     "metadata": {},
     "execution_count": 12
    }
   ],
   "source": [
    "# Show code from algorithms.py\n",
    "from IPython.display import Code\n",
    "filename = \"src\\\\algorithms.py\"\n",
    "with open(filename, 'r') as filehandle:\n",
    "    filecontent = filehandle.read()\n",
    "\n",
    "Code(filecontent)"
   ]
  }
 ]
}